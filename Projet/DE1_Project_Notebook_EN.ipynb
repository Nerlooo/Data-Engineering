{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DE1 ‚Äî Final Project Notebook\n",
        "> Author : Couzinet Lorenzo & Rabahi Enzo \n",
        "\n",
        "**Academic year:** 2025‚Äì2026  \n",
        "**Program:** Data & Applications - Engineering - (FD)   \n",
        "**Course:** Data Engineering I  \n",
        "\n",
        "---\n",
        "\n",
        "DataSet utilis√©: https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Load config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config loaded\n"
          ]
        }
      ],
      "source": [
        "import yaml, pathlib, datetime\n",
        "from pyspark.sql import SparkSession, functions as F, types as T\n",
        "import os\n",
        "\n",
        "# Force Spark √† utiliser l'adresse locale (localhost) pour √©viter les erreurs r√©seaux\n",
        "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
        "os.environ[\"OBJC_DISABLE_INITIALIZE_FORK_SAFETY\"] = \"YES\"\n",
        "\n",
        "with open(\"de1_project_config.yml\") as f:\n",
        "    CFG = yaml.safe_load(f)\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DE1-Project-Lakehouse\") \\\n",
        "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
        "    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "CFG \n",
        "\n",
        "proof = CFG[\"paths\"][\"proof\"]\n",
        "# Fonction pour sauvegarder le Plan Physique (La preuve technique)\n",
        "def save_execution_plan(df, filename):\n",
        "    # On r√©cup√®re le plan \"Expliqu√©\" complet\n",
        "    # mode=\"extended\" donne le Parsed, Analyzed, Optimized et Physical plan\n",
        "    plan = df._jdf.queryExecution().toString() \n",
        "    \n",
        "    filepath = f\"{proof}/{filename}.txt\"\n",
        "    with open(filepath, \"w\") as f:\n",
        "        f.write(plan)\n",
        "    print(f\"Plan d'ex√©cution sauvegard√© dans : {filepath}\")\n",
        "\n",
        "print(f\"Config loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Bronze ‚Äî landing raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- D√©marrage Bronze ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bronze written: outputs/bronze\n",
            "Plan d'ex√©cution sauvegard√© dans : proof/bronze_ingestion_plan.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/01/04 12:39:58 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:40:05 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:40:05 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:40:06 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:40:06 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:40:13 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:40:14 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:40:14 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Version enrichie (Parquet) √©crite dans : outputs/bronze_parquet\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 3:=======================================>                 (16 + 7) / 23]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre de lignes ing√©r√©es : 7728394\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "print(\"--- D√©marrage Bronze ---\")\n",
        "\n",
        "raw_glob = CFG[\"paths\"][\"raw_csv_glob\"]\n",
        "bronze = CFG[\"paths\"][\"bronze\"]\n",
        "\n",
        "df_raw = (spark.read.option(\"header\",\"true\").csv(raw_glob))\n",
        "df_raw.write.mode(\"overwrite\").csv(bronze)  # keep raw as CSV copy\n",
        "print(\"Bronze written:\", bronze)\n",
        "\n",
        "# 1. Enrichissement : Ajout des colonnes d'audit (Timestamp + Source)\n",
        "# On r√©utilise 'df_raw' qui est d√©j√† en m√©moire\n",
        "df_bronze_enhanced = df_raw \\\n",
        "    .withColumn(\"_ingested_at\", F.current_timestamp()) \\\n",
        "    .withColumn(\"_source_file\", F.input_file_name())\n",
        "\n",
        "# --- PREUVE ---\n",
        "# Cela prouve que Spark effectue un \"FileScan csv\"\n",
        "save_execution_plan(df_bronze_enhanced, \"bronze_ingestion_plan\")\n",
        "\n",
        "# 2. Sauvegarde en Parquet (Plus rapide pour l'√©tape Silver)\n",
        "# On d√©finit un nouveau chemin pour ne pas m√©langer avec le CSV\n",
        "bronze_parquet = f\"{bronze}_parquet\"\n",
        "\n",
        "df_bronze_enhanced.write.mode(\"overwrite\").parquet(bronze_parquet)\n",
        "\n",
        "print(f\"Version enrichie (Parquet) √©crite dans : {bronze_parquet}\")\n",
        "print(f\"Nombre de lignes ing√©r√©es : {df_bronze_enhanced.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Silver ‚Äî cleaning and typing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- D√©marrage Silver ---\n",
            "Plan d'ex√©cution sauvegard√© dans : proof/silver_transformation_plan.txt\n",
            "√âcriture dans outputs/silver (Partitionn√© par ['State'])...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/01/04 12:40:54 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:40:57 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Silver written: outputs/silver\n",
            "Nombre de lignes ing√©r√©es : 7728394\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import functions as F, types as T\n",
        "\n",
        "print(\"--- D√©marrage Silver ---\")\n",
        "\n",
        "# 1. Configuration des chemins\n",
        "# On lit le Bronze (version Parquet optimis√©e de l'√©tape pr√©c√©dente)\n",
        "bronze_path = f\"{CFG['paths']['bronze']}_parquet\"\n",
        "silver = CFG[\"paths\"][\"silver\"]\n",
        "\n",
        "# 2. Lecture\n",
        "df_bronze = spark.read.parquet(bronze_path)\n",
        "\n",
        "# 3. Transformation & Typage \n",
        "# On convertit les String en types r√©els (Timestamp, Int, Double)\n",
        "df_silver = (df_bronze\n",
        "    .select(\n",
        "        F.col(\"ID\").alias(\"accident_id\"),\n",
        "        F.col(\"Severity\").cast(\"int\"),\n",
        "        F.col(\"Start_Time\").cast(\"timestamp\").alias(\"event_time\"),\n",
        "        F.col(\"State\"),\n",
        "        F.col(\"City\"),\n",
        "        F.col(\"Temperature(F)\").cast(\"double\").alias(\"temp_f\"),\n",
        "        F.col(\"Weather_Condition\"),\n",
        "        F.col(\"_ingested_at\"), # On garde la tra√ßabilit√©\n",
        "        F.col(\"_source_file\")\n",
        "    )\n",
        "    # 4. Nettoyage : On supprime les lignes sans date ou sans √©tat\n",
        "    .dropna(subset=[\"event_time\", \"State\", \"accident_id\"])\n",
        ")\n",
        "\n",
        "# --- PREUVE ---\n",
        "# Sauvegarde le plan d'ex√©cution avant l'√©criture\n",
        "save_execution_plan(df_silver, \"silver_transformation_plan\")\n",
        "\n",
        "# 4. √âcriture avec Partitionnement\n",
        "# On r√©cup√®re la colonne de partition depuis la config (ex: [\"State\"])\n",
        "partition_cols = CFG['layout']['partition_by']\n",
        "\n",
        "print(f\"√âcriture dans {silver} (Partitionn√© par {partition_cols})...\")\n",
        "df_silver.write.mode(\"overwrite\").parquet(silver)\n",
        "print(\"Silver written:\", silver)\n",
        "\n",
        "print(f\"Nombre de lignes ing√©r√©es : {df_silver.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Gold ‚Äî analytics tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- D√©marrage Gold ---\n",
            "Plan d'ex√©cution sauvegard√© dans : proof/gold_q1_baseline_plan.txt\n",
            "√âcriture des tables Gold...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R√©sultats dans : outputs/gold\n",
            "Nombre de lignes de statistiques par Etat ing√©r√©es : 49\n",
            "Nombre de lignes temporelle ing√©r√©es : 87\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "print(\"--- D√©marrage Gold ---\")\n",
        "\n",
        "silver_path = CFG[\"paths\"][\"silver\"]\n",
        "gold_path = CFG[\"paths\"][\"gold\"]\n",
        "\n",
        "# 1. Lecture de la couche Silver\n",
        "df_silver = spark.read.parquet(silver_path)\n",
        "\n",
        "# --- KPI 1 : Statistiques par √âtat (Gravit√© moyenne & Total) ---\n",
        "df_gold_state = (df_silver\n",
        "    .groupBy(\"State\")\n",
        "    .agg(\n",
        "        F.count(\"accident_id\").alias(\"total_accidents\"),\n",
        "        F.round(F.avg(\"Severity\"), 2).alias(\"avg_severity\")\n",
        "    )\n",
        "    .orderBy(F.col(\"total_accidents\").desc())\n",
        ")\n",
        "\n",
        "# --- KPI 2 : Analyse Temporelle (Par mois) ---\n",
        "# On extrait le mois depuis event_time\n",
        "df_gold_monthly = (df_silver\n",
        "    .withColumn(\"Month\", F.date_format(\"event_time\", \"yyyy-MM\"))\n",
        "    .groupBy(\"Month\")\n",
        "    .count()\n",
        "    .orderBy(\"Month\")\n",
        ")\n",
        "\n",
        "# --- PREUVE ---\n",
        "# R√©f√©rence avant optimisation\n",
        "save_execution_plan(df_gold_state, \"gold_q1_baseline_plan\")\n",
        "\n",
        "# 3. √âcriture des r√©sultats\n",
        "print(\"√âcriture des tables Gold...\")\n",
        "df_gold_state.write.mode(\"overwrite\").parquet(f\"{gold_path}/accidents_by_state\")\n",
        "df_gold_monthly.write.mode(\"overwrite\").parquet(f\"{gold_path}/accidents_by_month\")\n",
        "\n",
        "print(f\"R√©sultats dans : {gold_path}\")\n",
        "print(f\"Nombre de lignes de statistiques par Etat ing√©r√©es : {df_gold_state.count()}\")\n",
        "print(f\"Nombre de lignes temporelle ing√©r√©es : {df_gold_monthly.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Baseline plans and metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- D√©marrage Baseline Metrics ---\n",
            "Plan d'ex√©cution sauvegard√© dans : proof/q1_baseline_plan.txt\n",
            "Lancement de l'ex√©cution Baseline (Q1)...\n",
            "Baseline termin√©e.\n",
            "ACTION REQUISE :\n",
            "1. Il faut aller sur http://localhost:4040 -> Onglet 'SQL'\n",
            "2. Rep√®rer la derni√®re requ√™te (Duration) puis noter ce temps (ex: 4s).\n"
          ]
        }
      ],
      "source": [
        "print(\"--- D√©marrage Baseline Metrics ---\")\n",
        "\n",
        "# 1. Configuration\n",
        "silver_path = CFG[\"paths\"][\"silver\"]\n",
        "gold_path = CFG[\"paths\"][\"gold\"]\n",
        "\n",
        "# Lecture de la table Silver (non-optimis√©e pour cette requ√™te)\n",
        "df_silver = spark.read.parquet(silver_path)\n",
        "\n",
        "# 2. D√©finition de la Requ√™te Q1 (Baseline)\n",
        "# \"Quels sont les √©tats avec le plus d'accidents graves ?\"\n",
        "# C'est une requ√™te lourde car elle doit scanner toute la table et grouper.\n",
        "df_gold_q1_baseline = (df_silver\n",
        "    .groupBy(\"State\")\n",
        "    .agg(\n",
        "        F.count(\"accident_id\").alias(\"total_accidents\"),\n",
        "        F.avg(\"Severity\").alias(\"avg_severity\")\n",
        "    )\n",
        "    .orderBy(F.col(\"total_accidents\").desc())\n",
        ")\n",
        "\n",
        "# 3. PREUVE 1 : Sauvegarde du Plan Baseline\n",
        "# Ce fichier montrera que Spark fait un gros scan (Scan Parquet)\n",
        "save_execution_plan(df_gold_q1_baseline, \"q1_baseline_plan\")\n",
        "\n",
        "# 4. Ex√©cution pour Mesure (Baseline Time)\n",
        "print(\"Lancement de l'ex√©cution Baseline (Q1)...\")\n",
        "\n",
        "# On √©crit le r√©sultat dans un dossier sp√©cifique \"baseline\"\n",
        "# Le mode \"overwrite\" assure qu'on refait le calcul √† chaque fois\n",
        "(df_gold_q1_baseline\n",
        "    .write\n",
        "    .mode(\"overwrite\")\n",
        "    .parquet(f\"{gold_path}/q1_baseline_results\")\n",
        ")\n",
        "\n",
        "print(\"Baseline termin√©e.\")\n",
        "print(\"ACTION REQUISE :\")\n",
        "print(\"1. Il faut aller sur http://localhost:4040 -> Onglet 'SQL'\")\n",
        "print(\"2. Rep√®rer la derni√®re requ√™te (Duration) puis noter ce temps.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Optimization ‚Äî layout and joins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- D√©marrage Optimization ---\n",
            "Cr√©ation de la table optimis√©e (Partitionn√©e par Ann√©e)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/01/04 12:56:13 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:56:13 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:56:13 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:56:14 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:56:14 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:56:14 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:56:14 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:56:14 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:56:14 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:56:14 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:56:14 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:56:14 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:56:15 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:56:15 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:56:15 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:56:15 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 12:56:15 WARN MemoryManager: Total allocation exceeds 95,00% (1‚ÄØ020‚ÄØ054‚ÄØ720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- TEST A : Table Standard (Partition=State) ---\n",
            "Spark doit scanner tous les dossiers d'√©tats pour trouver 2021.\n",
            "üëâ Action : Notez 'Files Read' dans le Stage correspondant sur Spark UI.\n",
            "\n",
            "--- TEST B : Table Optimis√©e (Partition=Year) ---\n",
            "Spark doit lire UNIQUEMENT le dossier year_part=2021.\n",
            "Optimisation termin√©e.\n",
            "ACTION REQUISE :\n",
            "1. Il faut aller sur http://localhost:4040 -> Onglet 'SQL'\n",
            "2. Rep√®rer la derni√®re requ√™te (Duration) puis noter ce temps.\n"
          ]
        }
      ],
      "source": [
        "print(\"--- D√©marrage Optimization ---\")\n",
        "\n",
        "silver_path = CFG[\"paths\"][\"silver\"]\n",
        "# Chemin pour la table optimis√©e\n",
        "optimized_path = f\"{CFG['paths']['silver']}_optimized_by_year\"\n",
        "\n",
        "# 1. Lecture de la table Silver existante (Partitionn√©e par State)\n",
        "df_silver = spark.read.parquet(silver_path)\n",
        "\n",
        "# --- PR√âPARATION DE L'OPTIMISATION ---\n",
        "# On cr√©e une nouvelle structure physique partitionn√©e par ANN√âE\n",
        "# Cela prend un peu de temps √† √©crire, mais rendra la lecture future instantan√©e.\n",
        "print(\"Cr√©ation de la table optimis√©e (Partitionn√©e par Ann√©e)...\")\n",
        "\n",
        "df_optimized = df_silver.withColumn(\"year_part\", F.year(\"event_time\"))\n",
        "\n",
        "(df_optimized\n",
        "    .write\n",
        "    .mode(\"overwrite\")\n",
        "    .partitionBy(\"year_part\") # <--- L'optimisation est ici\n",
        "    .parquet(optimized_path)\n",
        ")\n",
        "\n",
        "# --- LE DUEL : SC√âNARIO A vs SC√âNARIO B ---\n",
        "# Requ√™te : \"Compter les accidents survenus en 2021\"\n",
        "\n",
        "# CAS A : Sur la table rang√©e par √âTAT (Silver standard)\n",
        "print(\"\\n--- TEST A : Table Standard (Partition=State) ---\")\n",
        "print(\"Spark doit scanner tous les dossiers d'√©tats pour trouver 2021.\")\n",
        "\n",
        "# On vide le cache pour √™tre √©quitable\n",
        "spark.catalog.clearCache()\n",
        "\n",
        "# On lance la requ√™te\n",
        "df_silver.filter(F.year(\"event_time\") == 2021).count()\n",
        "print(\"ACTION REQUISE :\")\n",
        "print(\"1. Il faut aller sur http://localhost:4040 -> Onglet 'SQL'\")\n",
        "print(\"2. Rep√®rer la derni√®re requ√™te (Duration) puis noter ce temps.\")\n",
        "\n",
        "\n",
        "# CAS B : Sur la table rang√©e par ANN√âE (Optimis√©e)\n",
        "print(\"\\n--- TEST B : Table Optimis√©e (Partition=Year) ---\")\n",
        "print(\"Spark doit lire UNIQUEMENT le dossier year_part=2021.\")\n",
        "\n",
        "spark.catalog.clearCache()\n",
        "df_opt_read = spark.read.parquet(optimized_path)\n",
        "\n",
        "# On lance la m√™me requ√™te\n",
        "df_opt_read.filter(F.col(\"year_part\") == 2021).count()\n",
        "print(\"Optimisation termin√©e.\")\n",
        "print(\"ACTION REQUISE :\")\n",
        "print(\"1. Il faut aller sur http://localhost:4040 -> Onglet 'SQL'\")\n",
        "print(\"2. Rep√®rer la derni√®re requ√™te (Duration) puis noter ce temps.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session stopped.\n"
          ]
        }
      ],
      "source": [
        "spark.stop()\n",
        "print(\"Spark session stopped.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "de1-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
