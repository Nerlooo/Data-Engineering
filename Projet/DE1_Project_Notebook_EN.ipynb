{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DE1 — Final Project Notebook\n",
        "> Author : Couzinet Lorenzo & Rabahi Enzo \n",
        "\n",
        "**Academic year:** 2025–2026  \n",
        "**Program:** Data & Applications - Engineering - (FD)   \n",
        "**Course:** Data Engineering I  \n",
        "\n",
        "---\n",
        "\n",
        "DataSet utilisé: https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Load config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config loaded\n"
          ]
        }
      ],
      "source": [
        "import yaml, pathlib, datetime\n",
        "from pyspark.sql import SparkSession, functions as F, types as T\n",
        "import os\n",
        "\n",
        "# Force Spark à utiliser l'adresse locale (localhost) pour éviter les erreurs réseaux\n",
        "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
        "os.environ[\"OBJC_DISABLE_INITIALIZE_FORK_SAFETY\"] = \"YES\"\n",
        "\n",
        "with open(\"de1_project_config.yml\") as f:\n",
        "    CFG = yaml.safe_load(f)\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DE1-Project-Lakehouse\") \\\n",
        "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
        "    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "CFG \n",
        "\n",
        "proof = CFG[\"paths\"][\"proof\"]\n",
        "# Fonction pour sauvegarder le Plan Physique (La preuve technique)\n",
        "def save_execution_plan(df, filename):\n",
        "    # On récupère le plan \"Expliqué\" complet\n",
        "    # mode=\"extended\" donne le Parsed, Analyzed, Optimized et Physical plan\n",
        "    plan = df._jdf.queryExecution().toString() \n",
        "    \n",
        "    filepath = f\"{proof}/{filename}.txt\"\n",
        "    with open(filepath, \"w\") as f:\n",
        "        f.write(plan)\n",
        "    print(f\"Plan d'exécution sauvegardé dans : {filepath}\")\n",
        "\n",
        "print(f\"Config loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Bronze — landing raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Démarrage Bronze ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bronze written: outputs/project/bronze\n",
            "Plan d'exécution sauvegardé dans : proof/bronze_ingestion_plan.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/01/04 17:32:27 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:32:36 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:32:37 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:32:37 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:32:38 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:32:44 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:32:45 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:32:46 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:32:46 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:32:46 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:32:46 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Version enrichie (Parquet) écrite dans : outputs/project/bronze_parquet\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 3:=======================================>                 (16 + 7) / 23]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre de lignes ingérées : 7728394\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "print(\"--- Démarrage Bronze ---\")\n",
        "\n",
        "raw_glob = CFG[\"paths\"][\"raw_csv_glob\"]\n",
        "bronze = CFG[\"paths\"][\"bronze\"]\n",
        "\n",
        "df_raw = (spark.read.option(\"header\",\"true\").csv(raw_glob))\n",
        "df_raw.write.mode(\"overwrite\").csv(bronze)  # keep raw as CSV copy\n",
        "print(\"Bronze written:\", bronze)\n",
        "\n",
        "# 1. Enrichissement : Ajout des colonnes d'audit (Timestamp + Source)\n",
        "# On réutilise 'df_raw' qui est déjà en mémoire\n",
        "df_bronze_enhanced = df_raw \\\n",
        "    .withColumn(\"_ingested_at\", F.current_timestamp()) \\\n",
        "    .withColumn(\"_source_file\", F.input_file_name())\n",
        "\n",
        "# --- PREUVE ---\n",
        "# Cela prouve que Spark effectue un \"FileScan csv\"\n",
        "save_execution_plan(df_bronze_enhanced, \"bronze_ingestion_plan\")\n",
        "\n",
        "# 2. Sauvegarde en Parquet (Plus rapide pour l'étape Silver)\n",
        "# On définit un nouveau chemin pour ne pas mélanger avec le CSV\n",
        "bronze_parquet = f\"{bronze}_parquet\"\n",
        "\n",
        "df_bronze_enhanced.write.mode(\"overwrite\").parquet(bronze_parquet)\n",
        "\n",
        "print(f\"Version enrichie (Parquet) écrite dans : {bronze_parquet}\")\n",
        "print(f\"Nombre de lignes ingérées : {df_bronze_enhanced.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Silver — cleaning and typing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Démarrage Silver ---\n",
            "Plan d'exécution sauvegardé dans : proof/silver_transformation_plan.txt\n",
            "Écriture dans outputs/project/silver (Partitionné par ['State'])...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/01/04 17:33:46 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Silver written: outputs/project/silver\n",
            "Nombre de lignes ingérées : 7728394\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import functions as F, types as T\n",
        "\n",
        "print(\"--- Démarrage Silver ---\")\n",
        "\n",
        "# 1. Configuration des chemins\n",
        "# On lit le Bronze (version Parquet optimisée de l'étape précédente)\n",
        "bronze_path = f\"{CFG['paths']['bronze']}_parquet\"\n",
        "silver = CFG[\"paths\"][\"silver\"]\n",
        "\n",
        "# 2. Lecture\n",
        "df_bronze = spark.read.parquet(bronze_path)\n",
        "\n",
        "# 3. Transformation & Typage \n",
        "# On convertit les String en types réels (Timestamp, Int, Double)\n",
        "df_silver = (df_bronze\n",
        "    .select(\n",
        "        F.col(\"ID\").alias(\"accident_id\"),\n",
        "        F.col(\"Severity\").cast(\"int\"),\n",
        "        F.col(\"Start_Time\").cast(\"timestamp\").alias(\"event_time\"),\n",
        "        F.col(\"State\"),\n",
        "        F.col(\"City\"),\n",
        "        F.col(\"Temperature(F)\").cast(\"double\").alias(\"temp_f\"),\n",
        "        F.col(\"Weather_Condition\"),\n",
        "        F.col(\"_ingested_at\"), # On garde la traçabilité\n",
        "        F.col(\"_source_file\")\n",
        "    )\n",
        "    # 4. Nettoyage : On supprime les lignes sans date ou sans état\n",
        "    .dropna(subset=[\"event_time\", \"State\", \"accident_id\"])\n",
        ")\n",
        "\n",
        "# --- PREUVE ---\n",
        "# Sauvegarde le plan d'exécution avant l'écriture\n",
        "save_execution_plan(df_silver, \"silver_transformation_plan\")\n",
        "\n",
        "# 4. Écriture avec Partitionnement\n",
        "# On récupère la colonne de partition depuis la config (ex: [\"State\"])\n",
        "partition_cols = CFG['layout']['partition_by']\n",
        "\n",
        "print(f\"Écriture dans {silver} (Partitionné par {partition_cols})...\")\n",
        "df_silver.write.mode(\"overwrite\").parquet(silver)\n",
        "print(\"Silver written:\", silver)\n",
        "\n",
        "print(f\"Nombre de lignes ingérées : {df_silver.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Gold — analytics tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Démarrage Gold ---\n",
            "Plan d'exécution sauvegardé dans : proof/gold_q1_baseline_plan.txt\n",
            "Écriture des tables Gold...\n",
            "Résultats dans : outputs/project/gold\n",
            "Nombre de lignes de statistiques par Etat ingérées : 49\n",
            "Nombre de lignes temporelle ingérées : 87\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Démarrage Gold ---\")\n",
        "\n",
        "silver_path = CFG[\"paths\"][\"silver\"]\n",
        "gold_path = CFG[\"paths\"][\"gold\"]\n",
        "\n",
        "# 1. Lecture de la couche Silver\n",
        "df_silver = spark.read.parquet(silver_path)\n",
        "\n",
        "# --- KPI 1 : Statistiques par État (Gravité moyenne & Total) ---\n",
        "df_gold_state = (df_silver\n",
        "    .groupBy(\"State\")\n",
        "    .agg(\n",
        "        F.count(\"accident_id\").alias(\"total_accidents\"),\n",
        "        F.round(F.avg(\"Severity\"), 2).alias(\"avg_severity\")\n",
        "    )\n",
        "    .orderBy(F.col(\"total_accidents\").desc())\n",
        ")\n",
        "\n",
        "# --- KPI 2 : Analyse Temporelle (Par mois) ---\n",
        "# On extrait le mois depuis event_time\n",
        "df_gold_monthly = (df_silver\n",
        "    .withColumn(\"Month\", F.date_format(\"event_time\", \"yyyy-MM\"))\n",
        "    .groupBy(\"Month\")\n",
        "    .count()\n",
        "    .orderBy(\"Month\")\n",
        ")\n",
        "\n",
        "# --- PREUVE ---\n",
        "# Référence avant optimisation\n",
        "save_execution_plan(df_gold_state, \"gold_q1_baseline_plan\")\n",
        "\n",
        "# 3. Écriture des résultats\n",
        "print(\"Écriture des tables Gold...\")\n",
        "df_gold_state.write.mode(\"overwrite\").parquet(f\"{gold_path}/accidents_by_state\")\n",
        "df_gold_monthly.write.mode(\"overwrite\").parquet(f\"{gold_path}/accidents_by_month\")\n",
        "\n",
        "print(f\"Résultats dans : {gold_path}\")\n",
        "print(f\"Nombre de lignes de statistiques par Etat ingérées : {df_gold_state.count()}\")\n",
        "print(f\"Nombre de lignes temporelle ingérées : {df_gold_monthly.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Baseline plans and metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Démarrage Baseline Metrics ---\n",
            "Plan d'exécution sauvegardé dans : proof/q1_baseline_plan.txt\n",
            "Lancement de l'exécution Baseline (Q1)...\n",
            "Baseline terminée.\n",
            "ACTION REQUISE :\n",
            "1. Il faut aller sur http://localhost:4040 -> Onglet 'SQL'\n",
            "2. Repèrer la dernière requête (Duration) puis noter ce temps.\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Démarrage Baseline Metrics ---\")\n",
        "\n",
        "# 1. Configuration\n",
        "silver_path = CFG[\"paths\"][\"silver\"]\n",
        "gold_path = CFG[\"paths\"][\"gold\"]\n",
        "\n",
        "# Lecture de la table Silver (non-optimisée pour cette requête)\n",
        "df_silver = spark.read.parquet(silver_path)\n",
        "\n",
        "# 2. Définition de la Requête Q1 (Baseline)\n",
        "# \"Quels sont les états avec le plus d'accidents graves ?\"\n",
        "# C'est une requête lourde car elle doit scanner toute la table et grouper.\n",
        "df_gold_q1_baseline = (df_silver\n",
        "    .groupBy(\"State\")\n",
        "    .agg(\n",
        "        F.count(\"accident_id\").alias(\"total_accidents\"),\n",
        "        F.avg(\"Severity\").alias(\"avg_severity\")\n",
        "    )\n",
        "    .orderBy(F.col(\"total_accidents\").desc())\n",
        ")\n",
        "\n",
        "# 3. PREUVE 1 : Sauvegarde du Plan Baseline\n",
        "# Ce fichier montrera que Spark fait un gros scan (Scan Parquet)\n",
        "save_execution_plan(df_gold_q1_baseline, \"q1_baseline_plan\")\n",
        "\n",
        "# 4. Exécution pour Mesure (Baseline Time)\n",
        "print(\"Lancement de l'exécution Baseline (Q1)...\")\n",
        "\n",
        "# On écrit le résultat dans un dossier spécifique \"baseline\"\n",
        "# Le mode \"overwrite\" assure qu'on refait le calcul à chaque fois\n",
        "(df_gold_q1_baseline\n",
        "    .write\n",
        "    .mode(\"overwrite\")\n",
        "    .parquet(f\"{gold_path}/q1_baseline_results\")\n",
        ")\n",
        "\n",
        "print(\"Baseline terminée.\")\n",
        "print(\"ACTION REQUISE :\")\n",
        "print(\"1. Il faut aller sur http://localhost:4040 -> Onglet 'SQL'\")\n",
        "print(\"2. Repèrer la dernière requête (Duration) puis noter ce temps.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Optimization — layout and joins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Démarrage Optimization ---\n",
            "Création de la table optimisée (Partitionnée par Année)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/01/04 17:34:22 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:34:22 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:34:23 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:34:23 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:34:23 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:34:23 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:34:23 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:34:24 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:34:24 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:34:24 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:34:24 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "26/01/04 17:34:24 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\n",
            "Scaling row group sizes to 95,00% for 8 writers\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- TEST A : Table Standard (Partition=State) ---\n",
            "Spark doit scanner tous les dossiers d'états pour trouver 2021.\n",
            "ACTION REQUISE :\n",
            "1. Il faut aller sur http://localhost:4040 -> Onglet 'SQL'\n",
            "2. Repèrer l'avant dernière requête (Duration) puis noter ce temps.\n",
            "\n",
            "--- TEST B : Table Optimisée (Partition=Year) ---\n",
            "Spark doit lire UNIQUEMENT le dossier year_part=2021.\n",
            "Optimisation terminée.\n",
            "ACTION REQUISE :\n",
            "1. Il faut aller sur http://localhost:4040 -> Onglet 'SQL'\n",
            "2. Repèrer la dernière requête (Duration) puis noter ce temps.\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Démarrage Optimization ---\")\n",
        "\n",
        "silver_path = CFG[\"paths\"][\"silver\"]\n",
        "# Chemin pour la table optimisée\n",
        "optimized_path = f\"{CFG['paths']['silver']}_optimized_by_year\"\n",
        "\n",
        "# 1. Lecture de la table Silver existante (Partitionnée par State)\n",
        "df_silver = spark.read.parquet(silver_path)\n",
        "\n",
        "# --- PRÉPARATION DE L'OPTIMISATION ---\n",
        "# On crée une nouvelle structure physique partitionnée par ANNÉE\n",
        "# Cela prend un peu de temps à écrire, mais rendra la lecture future instantanée.\n",
        "print(\"Création de la table optimisée (Partitionnée par Année)...\")\n",
        "\n",
        "df_optimized = df_silver.withColumn(\"year_part\", F.year(\"event_time\"))\n",
        "\n",
        "(df_optimized\n",
        "    .write\n",
        "    .mode(\"overwrite\")\n",
        "    .partitionBy(\"year_part\") # <--- L'optimisation est ici\n",
        "    .parquet(optimized_path)\n",
        ")\n",
        "\n",
        "# --- LE DUEL : SCÉNARIO A vs SCÉNARIO B ---\n",
        "# Requête : \"Compter les accidents survenus en 2021\"\n",
        "\n",
        "# CAS A : Sur la table rangée par ÉTAT (Silver standard)\n",
        "print(\"\\n--- TEST A : Table Standard (Partition=State) ---\")\n",
        "print(\"Spark doit scanner tous les dossiers d'états pour trouver 2021.\")\n",
        "\n",
        "# On vide le cache pour être équitable\n",
        "spark.catalog.clearCache()\n",
        "\n",
        "# On lance la requête\n",
        "df_silver.filter(F.year(\"event_time\") == 2021).count()\n",
        "print(\"ACTION REQUISE :\")\n",
        "print(\"1. Il faut aller sur http://localhost:4040 -> Onglet 'SQL'\")\n",
        "print(\"2. Repèrer l'avant dernière requête (Duration) puis noter ce temps.\")\n",
        "\n",
        "\n",
        "# CAS B : Sur la table rangée par ANNÉE (Optimisée)\n",
        "print(\"\\n--- TEST B : Table Optimisée (Partition=Year) ---\")\n",
        "print(\"Spark doit lire UNIQUEMENT le dossier year_part=2021.\")\n",
        "\n",
        "spark.catalog.clearCache()\n",
        "df_opt_read = spark.read.parquet(optimized_path)\n",
        "\n",
        "# On lance la même requête\n",
        "df_opt_read.filter(F.col(\"year_part\") == 2021).count()\n",
        "print(\"Optimisation terminée.\")\n",
        "print(\"ACTION REQUISE :\")\n",
        "print(\"1. Il faut aller sur http://localhost:4040 -> Onglet 'SQL'\")\n",
        "print(\"2. Repèrer la dernière requête (Duration) puis noter ce temps.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session stopped.\n"
          ]
        }
      ],
      "source": [
        "spark.stop()\n",
        "print(\"Spark session stopped.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "de1-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
