{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DE1 — Lab 2: PostgreSQL → Star Schema ETL\n",
        "> Author : Badr TAJINI - Data Engineering I - ESIEE 2025-2026\n",
        "---\n",
        "\n",
        "\n",
        "Execute all cells. Attach evidence and fill metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup and schemas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Using incubator modules: jdk.incubator.vector\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/10/23 13:16:56 WARN Utils: Your hostname, MacBook-Air-de-Lorenzo.local, resolves to a loopback address: 127.0.0.1; using 147.215.204.26 instead (on interface en0)\n",
            "25/10/23 13:16:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/10/23 13:16:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession, functions as F, types as T\n",
        "spark = SparkSession.builder.appName(\"de1-lab2\").getOrCreate()\n",
        "base = \"data/\"\n",
        "# Explicit schemas\n",
        "customers_schema = T.StructType([\n",
        "    T.StructField(\"customer_id\", T.IntegerType(), False),\n",
        "    T.StructField(\"name\", T.StringType(), True),\n",
        "    T.StructField(\"email\", T.StringType(), True),\n",
        "    T.StructField(\"created_at\", T.TimestampType(), True),\n",
        "])\n",
        "brands_schema = T.StructType([\n",
        "    T.StructField(\"brand_id\", T.IntegerType(), False),\n",
        "    T.StructField(\"brand_name\", T.StringType(), True),\n",
        "])\n",
        "categories_schema = T.StructType([\n",
        "    T.StructField(\"category_id\", T.IntegerType(), False),\n",
        "    T.StructField(\"category_name\", T.StringType(), True),\n",
        "])\n",
        "products_schema = T.StructType([\n",
        "    T.StructField(\"product_id\", T.IntegerType(), False),\n",
        "    T.StructField(\"product_name\", T.StringType(), True),\n",
        "    T.StructField(\"brand_id\", T.IntegerType(), True),\n",
        "    T.StructField(\"category_id\", T.IntegerType(), True),\n",
        "    T.StructField(\"price\", T.DoubleType(), True),\n",
        "])\n",
        "orders_schema = T.StructType([\n",
        "    T.StructField(\"order_id\", T.IntegerType(), False),\n",
        "    T.StructField(\"customer_id\", T.IntegerType(), True),\n",
        "    T.StructField(\"order_date\", T.TimestampType(), True),\n",
        "])\n",
        "order_items_schema = T.StructType([\n",
        "    T.StructField(\"order_item_id\", T.IntegerType(), False),\n",
        "    T.StructField(\"order_id\", T.IntegerType(), True),\n",
        "    T.StructField(\"product_id\", T.IntegerType(), True),\n",
        "    T.StructField(\"quantity\", T.IntegerType(), True),\n",
        "    T.StructField(\"unit_price\", T.DoubleType(), True),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Ingest operational tables (from CSV exports)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "customers 24\n",
            "brands 8\n",
            "categories 9\n",
            "products 60\n",
            "orders 220\n",
            "order_items 638\n"
          ]
        }
      ],
      "source": [
        "customers = spark.read.schema(customers_schema).option(\"header\",\"true\").csv(base+\"lab2_customers.csv\")\n",
        "brands = spark.read.schema(brands_schema).option(\"header\",\"true\").csv(base+\"lab2_brands.csv\")\n",
        "categories = spark.read.schema(categories_schema).option(\"header\",\"true\").csv(base+\"lab2_categories.csv\")\n",
        "products = spark.read.schema(products_schema).option(\"header\",\"true\").csv(base+\"lab2_products.csv\")\n",
        "orders = spark.read.schema(orders_schema).option(\"header\",\"true\").csv(base+\"lab2_orders.csv\")\n",
        "order_items = spark.read.schema(order_items_schema).option(\"header\",\"true\").csv(base+\"lab2_order_items.csv\")\n",
        "\n",
        "for name, df in [(\"customers\",customers),(\"brands\",brands),(\"categories\",categories),(\"products\",products),(\"orders\",orders),(\"order_items\",order_items)]:\n",
        "    print(name, df.count())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evidence: ingestion plan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan (11)\n",
            "+- HashAggregate (10)\n",
            "   +- Exchange (9)\n",
            "      +- HashAggregate (8)\n",
            "         +- Project (7)\n",
            "            +- BroadcastHashJoin Inner BuildLeft (6)\n",
            "               :- BroadcastExchange (3)\n",
            "               :  +- Filter (2)\n",
            "               :     +- Scan csv  (1)\n",
            "               +- Filter (5)\n",
            "                  +- Scan csv  (4)\n",
            "\n",
            "\n",
            "(1) Scan csv \n",
            "Output [1]: [order_id#13]\n",
            "Batched: false\n",
            "Location: InMemoryFileIndex [file:/Users/lorenzo/Documents/Cours/E4FD/NP Data Engineering/Lab2/Lab2Practice/data/lab2_orders.csv]\n",
            "PushedFilters: [IsNotNull(order_id)]\n",
            "ReadSchema: struct<order_id:int>\n",
            "\n",
            "(2) Filter\n",
            "Input [1]: [order_id#13]\n",
            "Condition : isnotnull(order_id#13)\n",
            "\n",
            "(3) BroadcastExchange\n",
            "Input [1]: [order_id#13]\n",
            "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=221]\n",
            "\n",
            "(4) Scan csv \n",
            "Output [1]: [order_id#17]\n",
            "Batched: false\n",
            "Location: InMemoryFileIndex [file:/Users/lorenzo/Documents/Cours/E4FD/NP Data Engineering/Lab2/Lab2Practice/data/lab2_order_items.csv]\n",
            "PushedFilters: [IsNotNull(order_id)]\n",
            "ReadSchema: struct<order_id:int>\n",
            "\n",
            "(5) Filter\n",
            "Input [1]: [order_id#17]\n",
            "Condition : isnotnull(order_id#17)\n",
            "\n",
            "(6) BroadcastHashJoin\n",
            "Left keys [1]: [order_id#13]\n",
            "Right keys [1]: [order_id#17]\n",
            "Join type: Inner\n",
            "Join condition: None\n",
            "\n",
            "(7) Project\n",
            "Output [1]: [order_id#13]\n",
            "Input [2]: [order_id#13, order_id#17]\n",
            "\n",
            "(8) HashAggregate\n",
            "Input [1]: [order_id#13]\n",
            "Keys [1]: [order_id#13]\n",
            "Functions: []\n",
            "Aggregate Attributes: []\n",
            "Results [1]: [order_id#13]\n",
            "\n",
            "(9) Exchange\n",
            "Input [1]: [order_id#13]\n",
            "Arguments: hashpartitioning(order_id#13, 200), ENSURE_REQUIREMENTS, [plan_id=226]\n",
            "\n",
            "(10) HashAggregate\n",
            "Input [1]: [order_id#13]\n",
            "Keys [1]: [order_id#13]\n",
            "Functions: []\n",
            "Aggregate Attributes: []\n",
            "Results [1]: [order_id#13]\n",
            "\n",
            "(11) AdaptiveSparkPlan\n",
            "Output [1]: [order_id#13]\n",
            "Arguments: isFinalPlan=false\n",
            "\n",
            "\n",
            "Saved proof/plan_ingest.txt\n"
          ]
        }
      ],
      "source": [
        "ingest = orders.join(order_items, \"order_id\").select(\"order_id\").distinct()\n",
        "ingest.explain(\"formatted\")\n",
        "from datetime import datetime as _dt\n",
        "import pathlib\n",
        "pathlib.Path(\"proof\").mkdir(exist_ok=True)\n",
        "with open(\"proof/plan_ingest.txt\",\"w\") as f:\n",
        "    f.write(str(_dt.now())+\"\\n\")\n",
        "    f.write(ingest._jdf.queryExecution().executedPlan().toString())\n",
        "print(\"Saved proof/plan_ingest.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Surrogate key function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sk(cols):\n",
        "    # stable 64-bit positive surrogate key from natural keys\n",
        "    return F.abs(F.xxhash64(*[F.col(c) for c in cols]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "dim_customer = customers.select(\n",
        "    sk([\"customer_id\"]).alias(\"customer_sk\"),\n",
        "    \"customer_id\",\"name\",\"email\",\"created_at\"\n",
        ")\n",
        "\n",
        "dim_brand = brands.select(\n",
        "    sk([\"brand_id\"]).alias(\"brand_sk\"),\n",
        "    \"brand_id\",\"brand_name\"\n",
        ")\n",
        "\n",
        "dim_category = categories.select(\n",
        "    sk([\"category_id\"]).alias(\"category_sk\"),\n",
        "    \"category_id\",\"category_name\"\n",
        ")\n",
        "\n",
        "dim_product = products.select(\n",
        "    sk([\"product_id\"]).alias(\"product_sk\"),\n",
        "    \"product_id\",\"product_name\",\n",
        "    sk([\"brand_id\"]).alias(\"brand_sk\"),\n",
        "    sk([\"category_id\"]).alias(\"category_sk\"),\n",
        "    \"price\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Build date dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import Window as W\n",
        "dates = orders.select(F.to_date(\"order_date\").alias(\"date\")).distinct()\n",
        "dim_date = dates.select(\n",
        "    sk([\"date\"]).alias(\"date_sk\"),\n",
        "    F.col(\"date\"),\n",
        "    F.year(\"date\").alias(\"year\"),\n",
        "    F.month(\"date\").alias(\"month\"),\n",
        "    F.dayofmonth(\"date\").alias(\"day\"),\n",
        "    F.date_format(\"date\",\"E\").alias(\"dow\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build fact_sales with broadcast joins where appropriate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan (19)\n",
            "+- Project (18)\n",
            "   +- Project (17)\n",
            "      +- BroadcastHashJoin Inner BuildRight (16)\n",
            "         :- Project (12)\n",
            "         :  +- BroadcastHashJoin Inner BuildRight (11)\n",
            "         :     :- Project (7)\n",
            "         :     :  +- BroadcastHashJoin Inner BuildRight (6)\n",
            "         :     :     :- Filter (2)\n",
            "         :     :     :  +- Scan csv  (1)\n",
            "         :     :     +- BroadcastExchange (5)\n",
            "         :     :        +- Filter (4)\n",
            "         :     :           +- Scan csv  (3)\n",
            "         :     +- BroadcastExchange (10)\n",
            "         :        +- Filter (9)\n",
            "         :           +- Scan csv  (8)\n",
            "         +- BroadcastExchange (15)\n",
            "            +- Filter (14)\n",
            "               +- Scan csv  (13)\n",
            "\n",
            "\n",
            "(1) Scan csv \n",
            "Output [4]: [order_id#17, product_id#18, quantity#19, unit_price#20]\n",
            "Batched: false\n",
            "Location: InMemoryFileIndex [file:/Users/lorenzo/Documents/Cours/E4FD/NP Data Engineering/Lab2/Lab2Practice/data/lab2_order_items.csv]\n",
            "PushedFilters: [IsNotNull(product_id), IsNotNull(order_id)]\n",
            "ReadSchema: struct<order_id:int,product_id:int,quantity:int,unit_price:double>\n",
            "\n",
            "(2) Filter\n",
            "Input [4]: [order_id#17, product_id#18, quantity#19, unit_price#20]\n",
            "Condition : (isnotnull(product_id#18) AND isnotnull(order_id#17))\n",
            "\n",
            "(3) Scan csv \n",
            "Output [1]: [product_id#8]\n",
            "Batched: false\n",
            "Location: InMemoryFileIndex [file:/Users/lorenzo/Documents/Cours/E4FD/NP Data Engineering/Lab2/Lab2Practice/data/lab2_products.csv]\n",
            "PushedFilters: [IsNotNull(product_id)]\n",
            "ReadSchema: struct<product_id:int>\n",
            "\n",
            "(4) Filter\n",
            "Input [1]: [product_id#8]\n",
            "Condition : isnotnull(product_id#8)\n",
            "\n",
            "(5) BroadcastExchange\n",
            "Input [1]: [product_id#8]\n",
            "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=295]\n",
            "\n",
            "(6) BroadcastHashJoin\n",
            "Left keys [1]: [product_id#18]\n",
            "Right keys [1]: [product_id#8]\n",
            "Join type: Inner\n",
            "Join condition: None\n",
            "\n",
            "(7) Project\n",
            "Output [4]: [order_id#17, product_id#18, quantity#19, unit_price#20]\n",
            "Input [5]: [order_id#17, product_id#18, quantity#19, unit_price#20, product_id#8]\n",
            "\n",
            "(8) Scan csv \n",
            "Output [3]: [order_id#13, customer_id#14, order_date#15]\n",
            "Batched: false\n",
            "Location: InMemoryFileIndex [file:/Users/lorenzo/Documents/Cours/E4FD/NP Data Engineering/Lab2/Lab2Practice/data/lab2_orders.csv]\n",
            "PushedFilters: [IsNotNull(order_id), IsNotNull(customer_id)]\n",
            "ReadSchema: struct<order_id:int,customer_id:int,order_date:timestamp>\n",
            "\n",
            "(9) Filter\n",
            "Input [3]: [order_id#13, customer_id#14, order_date#15]\n",
            "Condition : (isnotnull(order_id#13) AND isnotnull(customer_id#14))\n",
            "\n",
            "(10) BroadcastExchange\n",
            "Input [3]: [order_id#13, customer_id#14, order_date#15]\n",
            "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=299]\n",
            "\n",
            "(11) BroadcastHashJoin\n",
            "Left keys [1]: [order_id#17]\n",
            "Right keys [1]: [order_id#13]\n",
            "Join type: Inner\n",
            "Join condition: None\n",
            "\n",
            "(12) Project\n",
            "Output [6]: [order_id#17, product_id#18, quantity#19, unit_price#20, customer_id#14, order_date#15]\n",
            "Input [7]: [order_id#17, product_id#18, quantity#19, unit_price#20, order_id#13, customer_id#14, order_date#15]\n",
            "\n",
            "(13) Scan csv \n",
            "Output [1]: [customer_id#0]\n",
            "Batched: false\n",
            "Location: InMemoryFileIndex [file:/Users/lorenzo/Documents/Cours/E4FD/NP Data Engineering/Lab2/Lab2Practice/data/lab2_customers.csv]\n",
            "PushedFilters: [IsNotNull(customer_id)]\n",
            "ReadSchema: struct<customer_id:int>\n",
            "\n",
            "(14) Filter\n",
            "Input [1]: [customer_id#0]\n",
            "Condition : isnotnull(customer_id#0)\n",
            "\n",
            "(15) BroadcastExchange\n",
            "Input [1]: [customer_id#0]\n",
            "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=303]\n",
            "\n",
            "(16) BroadcastHashJoin\n",
            "Left keys [1]: [customer_id#14]\n",
            "Right keys [1]: [customer_id#0]\n",
            "Join type: Inner\n",
            "Join condition: None\n",
            "\n",
            "(17) Project\n",
            "Output [6]: [customer_id#14, order_id#17, product_id#18, quantity#19, unit_price#20, cast(order_date#15 as date) AS date#88]\n",
            "Input [7]: [order_id#17, product_id#18, quantity#19, unit_price#20, customer_id#14, order_date#15, customer_id#0]\n",
            "\n",
            "(18) Project\n",
            "Output [9]: [order_id#17, abs(xxhash64(date#88, 42)) AS date_sk#89L, abs(xxhash64(customer_id#14, 42)) AS customer_sk#90L, abs(xxhash64(product_id#18, 42)) AS product_sk#91L, quantity#19, unit_price#20, (cast(quantity#19 as double) * unit_price#20) AS subtotal#94, year(date#88) AS year#95, month(date#88) AS month#96]\n",
            "Input [6]: [customer_id#14, order_id#17, product_id#18, quantity#19, unit_price#20, date#88]\n",
            "\n",
            "(19) AdaptiveSparkPlan\n",
            "Output [9]: [order_id#17, date_sk#89L, customer_sk#90L, product_sk#91L, quantity#19, unit_price#20, subtotal#94, year#95, month#96]\n",
            "Arguments: isFinalPlan=false\n",
            "\n",
            "\n",
            "Saved proof/plan_fact_join.txt\n"
          ]
        }
      ],
      "source": [
        "oi = order_items.alias(\"oi\")\n",
        "p = products.alias(\"p\")\n",
        "o = orders.alias(\"o\")\n",
        "c = customers.alias(\"c\")\n",
        "\n",
        "# Join with small dimensions using DF copies to compute SKs, then broadcast dims by size heuristic\n",
        "df_fact = (oi\n",
        "    .join(p, F.col(\"oi.product_id\")==F.col(\"p.product_id\"))\n",
        "    .join(o, \"order_id\")\n",
        "    .join(c, \"customer_id\")\n",
        "    .withColumn(\"date\", F.to_date(\"order_date\"))\n",
        ")\n",
        "\n",
        "# Attach surrogate keys\n",
        "df_fact = (df_fact\n",
        "    .withColumn(\"date_sk\", sk([\"date\"]))\n",
        "    .withColumn(\"customer_sk\", sk([\"customer_id\"]))\n",
        "    .withColumn(\"product_sk\", sk([\"oi.product_id\"])) #MODIFICATION ICI\n",
        "    .withColumn(\"quantity\", F.col(\"quantity\").cast(\"int\"))\n",
        "    .withColumn(\"unit_price\", F.col(\"unit_price\").cast(\"double\"))\n",
        "    .withColumn(\"subtotal\", F.col(\"quantity\")*F.col(\"unit_price\"))\n",
        "    .withColumn(\"year\", F.year(\"date\"))\n",
        "    .withColumn(\"month\", F.month(\"date\"))\n",
        "    .select(\"order_id\",\"date_sk\",\"customer_sk\",\"product_sk\",\"quantity\",\"unit_price\",\"subtotal\",\"year\",\"month\")\n",
        ")\n",
        "\n",
        "df_fact.explain(\"formatted\")\n",
        "with open(\"proof/plan_fact_join.txt\",\"w\") as f:\n",
        "    from datetime import datetime as _dt\n",
        "    f.write(str(_dt.now())+\"\\n\")\n",
        "    f.write(df_fact._jdf.queryExecution().executedPlan().toString())\n",
        "print(\"Saved proof/plan_fact_join.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Write Parquet outputs (partitioned by year, month)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parquet written under outputs/lab2/\n"
          ]
        }
      ],
      "source": [
        "base_out = \"outputs/lab2\"\n",
        "(dim_customer.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_customer\"))\n",
        "(dim_brand.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_brand\"))\n",
        "(dim_category.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_category\"))\n",
        "(dim_product.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_product\"))\n",
        "(dim_date.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_date\"))\n",
        "(df_fact.write.mode(\"overwrite\").partitionBy(\"year\",\"month\").parquet(f\"{base_out}/fact_sales\"))\n",
        "print(\"Parquet written under outputs/lab2/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Plan comparison: projection and layout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan (16)\n",
            "+- HashAggregate (15)\n",
            "   +- Exchange (14)\n",
            "      +- HashAggregate (13)\n",
            "         +- Project (12)\n",
            "            +- BroadcastHashJoin Inner BuildRight (11)\n",
            "               :- Project (7)\n",
            "               :  +- BroadcastHashJoin Inner BuildLeft (6)\n",
            "               :     :- BroadcastExchange (3)\n",
            "               :     :  +- Filter (2)\n",
            "               :     :     +- Scan csv  (1)\n",
            "               :     +- Filter (5)\n",
            "               :        +- Scan csv  (4)\n",
            "               +- BroadcastExchange (10)\n",
            "                  +- Filter (9)\n",
            "                     +- Scan csv  (8)\n",
            "\n",
            "\n",
            "(1) Scan csv \n",
            "Output [2]: [order_id#13, order_date#15]\n",
            "Batched: false\n",
            "Location: InMemoryFileIndex [file:/Users/lorenzo/Documents/Cours/E4FD/NP Data Engineering/Lab2/Lab2Practice/data/lab2_orders.csv]\n",
            "PushedFilters: [IsNotNull(order_id)]\n",
            "ReadSchema: struct<order_id:int,order_date:timestamp>\n",
            "\n",
            "(2) Filter\n",
            "Input [2]: [order_id#13, order_date#15]\n",
            "Condition : isnotnull(order_id#13)\n",
            "\n",
            "(3) BroadcastExchange\n",
            "Input [2]: [order_id#13, order_date#15]\n",
            "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=770]\n",
            "\n",
            "(4) Scan csv \n",
            "Output [3]: [order_id#17, product_id#18, quantity#19]\n",
            "Batched: false\n",
            "Location: InMemoryFileIndex [file:/Users/lorenzo/Documents/Cours/E4FD/NP Data Engineering/Lab2/Lab2Practice/data/lab2_order_items.csv]\n",
            "PushedFilters: [IsNotNull(order_id), IsNotNull(product_id)]\n",
            "ReadSchema: struct<order_id:int,product_id:int,quantity:int>\n",
            "\n",
            "(5) Filter\n",
            "Input [3]: [order_id#17, product_id#18, quantity#19]\n",
            "Condition : (isnotnull(order_id#17) AND isnotnull(product_id#18))\n",
            "\n",
            "(6) BroadcastHashJoin\n",
            "Left keys [1]: [order_id#13]\n",
            "Right keys [1]: [order_id#17]\n",
            "Join type: Inner\n",
            "Join condition: None\n",
            "\n",
            "(7) Project\n",
            "Output [3]: [order_date#15, product_id#18, quantity#19]\n",
            "Input [5]: [order_id#13, order_date#15, order_id#17, product_id#18, quantity#19]\n",
            "\n",
            "(8) Scan csv \n",
            "Output [2]: [product_id#8, price#12]\n",
            "Batched: false\n",
            "Location: InMemoryFileIndex [file:/Users/lorenzo/Documents/Cours/E4FD/NP Data Engineering/Lab2/Lab2Practice/data/lab2_products.csv]\n",
            "PushedFilters: [IsNotNull(product_id)]\n",
            "ReadSchema: struct<product_id:int,price:double>\n",
            "\n",
            "(9) Filter\n",
            "Input [2]: [product_id#8, price#12]\n",
            "Condition : isnotnull(product_id#8)\n",
            "\n",
            "(10) BroadcastExchange\n",
            "Input [2]: [product_id#8, price#12]\n",
            "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=774]\n",
            "\n",
            "(11) BroadcastHashJoin\n",
            "Left keys [1]: [product_id#18]\n",
            "Right keys [1]: [product_id#8]\n",
            "Join type: Inner\n",
            "Join condition: None\n",
            "\n",
            "(12) Project\n",
            "Output [3]: [quantity#19, price#12, cast(order_date#15 as date) AS _groupingexpression#134]\n",
            "Input [5]: [order_date#15, product_id#18, quantity#19, product_id#8, price#12]\n",
            "\n",
            "(13) HashAggregate\n",
            "Input [3]: [quantity#19, price#12, _groupingexpression#134]\n",
            "Keys [1]: [_groupingexpression#134]\n",
            "Functions [1]: [partial_sum((cast(quantity#19 as double) * price#12))]\n",
            "Aggregate Attributes [1]: [sum#135]\n",
            "Results [2]: [_groupingexpression#134, sum#136]\n",
            "\n",
            "(14) Exchange\n",
            "Input [2]: [_groupingexpression#134, sum#136]\n",
            "Arguments: hashpartitioning(_groupingexpression#134, 200), ENSURE_REQUIREMENTS, [plan_id=779]\n",
            "\n",
            "(15) HashAggregate\n",
            "Input [2]: [_groupingexpression#134, sum#136]\n",
            "Keys [1]: [_groupingexpression#134]\n",
            "Functions [1]: [sum((cast(quantity#19 as double) * price#12))]\n",
            "Aggregate Attributes [1]: [sum((cast(quantity#19 as double) * price#12))#133]\n",
            "Results [2]: [_groupingexpression#134 AS d#120, sum((cast(quantity#19 as double) * price#12))#133 AS gmv#121]\n",
            "\n",
            "(16) AdaptiveSparkPlan\n",
            "Output [2]: [d#120, gmv#121]\n",
            "Arguments: isFinalPlan=false\n",
            "\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan (16)\n",
            "+- HashAggregate (15)\n",
            "   +- Exchange (14)\n",
            "      +- HashAggregate (13)\n",
            "         +- Project (12)\n",
            "            +- BroadcastHashJoin Inner BuildRight (11)\n",
            "               :- Project (7)\n",
            "               :  +- BroadcastHashJoin Inner BuildLeft (6)\n",
            "               :     :- BroadcastExchange (3)\n",
            "               :     :  +- Filter (2)\n",
            "               :     :     +- Scan csv  (1)\n",
            "               :     +- Filter (5)\n",
            "               :        +- Scan csv  (4)\n",
            "               +- BroadcastExchange (10)\n",
            "                  +- Filter (9)\n",
            "                     +- Scan csv  (8)\n",
            "\n",
            "\n",
            "(1) Scan csv \n",
            "Output [2]: [order_id#13, order_date#15]\n",
            "Batched: false\n",
            "Location: InMemoryFileIndex [file:/Users/lorenzo/Documents/Cours/E4FD/NP Data Engineering/Lab2/Lab2Practice/data/lab2_orders.csv]\n",
            "PushedFilters: [IsNotNull(order_id)]\n",
            "ReadSchema: struct<order_id:int,order_date:timestamp>\n",
            "\n",
            "(2) Filter\n",
            "Input [2]: [order_id#13, order_date#15]\n",
            "Condition : isnotnull(order_id#13)\n",
            "\n",
            "(3) BroadcastExchange\n",
            "Input [2]: [order_id#13, order_date#15]\n",
            "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1145]\n",
            "\n",
            "(4) Scan csv \n",
            "Output [3]: [order_id#17, product_id#18, quantity#19]\n",
            "Batched: false\n",
            "Location: InMemoryFileIndex [file:/Users/lorenzo/Documents/Cours/E4FD/NP Data Engineering/Lab2/Lab2Practice/data/lab2_order_items.csv]\n",
            "PushedFilters: [IsNotNull(order_id), IsNotNull(product_id)]\n",
            "ReadSchema: struct<order_id:int,product_id:int,quantity:int>\n",
            "\n",
            "(5) Filter\n",
            "Input [3]: [order_id#17, product_id#18, quantity#19]\n",
            "Condition : (isnotnull(order_id#17) AND isnotnull(product_id#18))\n",
            "\n",
            "(6) BroadcastHashJoin\n",
            "Left keys [1]: [order_id#13]\n",
            "Right keys [1]: [order_id#17]\n",
            "Join type: Inner\n",
            "Join condition: None\n",
            "\n",
            "(7) Project\n",
            "Output [3]: [order_date#15, product_id#18, quantity#19]\n",
            "Input [5]: [order_id#13, order_date#15, order_id#17, product_id#18, quantity#19]\n",
            "\n",
            "(8) Scan csv \n",
            "Output [2]: [product_id#8, price#12]\n",
            "Batched: false\n",
            "Location: InMemoryFileIndex [file:/Users/lorenzo/Documents/Cours/E4FD/NP Data Engineering/Lab2/Lab2Practice/data/lab2_products.csv]\n",
            "PushedFilters: [IsNotNull(product_id)]\n",
            "ReadSchema: struct<product_id:int,price:double>\n",
            "\n",
            "(9) Filter\n",
            "Input [2]: [product_id#8, price#12]\n",
            "Condition : isnotnull(product_id#8)\n",
            "\n",
            "(10) BroadcastExchange\n",
            "Input [2]: [product_id#8, price#12]\n",
            "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1149]\n",
            "\n",
            "(11) BroadcastHashJoin\n",
            "Left keys [1]: [product_id#18]\n",
            "Right keys [1]: [product_id#8]\n",
            "Join type: Inner\n",
            "Join condition: None\n",
            "\n",
            "(12) Project\n",
            "Output [3]: [quantity#19, price#12, cast(order_date#15 as date) AS _groupingexpression#157]\n",
            "Input [5]: [order_date#15, product_id#18, quantity#19, product_id#8, price#12]\n",
            "\n",
            "(13) HashAggregate\n",
            "Input [3]: [quantity#19, price#12, _groupingexpression#157]\n",
            "Keys [1]: [_groupingexpression#157]\n",
            "Functions [1]: [partial_sum((cast(quantity#19 as double) * price#12))]\n",
            "Aggregate Attributes [1]: [sum#158]\n",
            "Results [2]: [_groupingexpression#157, sum#159]\n",
            "\n",
            "(14) Exchange\n",
            "Input [2]: [_groupingexpression#157, sum#159]\n",
            "Arguments: hashpartitioning(_groupingexpression#157, 200), ENSURE_REQUIREMENTS, [plan_id=1154]\n",
            "\n",
            "(15) HashAggregate\n",
            "Input [2]: [_groupingexpression#157, sum#159]\n",
            "Keys [1]: [_groupingexpression#157]\n",
            "Functions [1]: [sum((cast(quantity#19 as double) * price#12))]\n",
            "Aggregate Attributes [1]: [sum((cast(quantity#19 as double) * price#12))#156]\n",
            "Results [2]: [_groupingexpression#157 AS d#149, sum((cast(quantity#19 as double) * price#12))#156 AS gmv#150]\n",
            "\n",
            "(16) AdaptiveSparkPlan\n",
            "Output [2]: [d#149, gmv#150]\n",
            "Arguments: isFinalPlan=false\n",
            "\n",
            "\n",
            "Record Spark UI metrics for both runs in lab2_metrics_log.csv\n"
          ]
        }
      ],
      "source": [
        "# Case A: join and then project\n",
        "a = (orders.join(order_items, \"order_id\")\n",
        "            .join(products, \"product_id\")\n",
        "            .groupBy(F.to_date(\"order_date\").alias(\"d\"))\n",
        "            .agg(F.sum(F.col(\"quantity\")*F.col(\"price\")).alias(\"gmv\")))\n",
        "a.explain(\"formatted\")\n",
        "_ = a.count()\n",
        "\n",
        "# Case B: project early\n",
        "b = (orders.select(\"order_id\",\"order_date\")\n",
        "            .join(order_items.select(\"order_id\",\"product_id\",\"quantity\"), \"order_id\")\n",
        "            .join(products.select(\"product_id\",\"price\"), \"product_id\")\n",
        "            .groupBy(F.to_date(\"order_date\").alias(\"d\"))\n",
        "            .agg(F.sum(F.col(\"quantity\")*F.col(\"price\")).alias(\"gmv\")))\n",
        "b.explain(\"formatted\")\n",
        "_ = b.count()\n",
        "\n",
        "print(\"Record Spark UI metrics for both runs in lab2_metrics_log.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session stopped.\n"
          ]
        }
      ],
      "source": [
        "spark.stop()\n",
        "print(\"Spark session stopped.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "de1-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
