{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a8e1df",
   "metadata": {},
   "source": [
    "# ESIEE Paris — Data Engineering I — Assignment 2\n",
    "> Author : Couzinet Lorenzo & Rabahi Enzo \n",
    "\n",
    "**Academic year:** 2025–2026  \n",
    "**Program:** Data & Applications - Engineering - (FD)   \n",
    "**Course:** Data Engineering I  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a13f5da",
   "metadata": {},
   "source": [
    "## Data inputs\n",
    "Define your input paths. Use small CSV/JSON/Parquet files so the notebook runs locally. If your dataset requires credentials, create a **sample subset** and commit only that.\n",
    "\n",
    "**Paths to set:**\n",
    "- `SOURCE_A_PATH` (fact‑like dataset)\n",
    "- `SOURCE_B_PATH` (dimension‑like dataset)\n",
    "- `OUTPUT_BASE` (directory for Parquet output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6377a4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source A: events.csv\n",
      "Source B: product.csv\n",
      "Output:   output\n"
     ]
    }
   ],
   "source": [
    "# Read carefully the helper to review what is missing here\n",
    "\n",
    "SOURCE_A_PATH = 'events.csv'\n",
    "SOURCE_B_PATH = 'product.csv'\n",
    "OUTPUT_BASE   = 'output'\n",
    "\n",
    "print(f\"Source A: {SOURCE_A_PATH}\")\n",
    "print(f\"Source B: {SOURCE_B_PATH}\")\n",
    "print(f\"Output:   {OUTPUT_BASE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c5ba49",
   "metadata": {},
   "source": [
    "## Pipeline API (implementations required)\n",
    "Implement the following functions. Keep signatures stable. Use explicit schemas when possible. Log counts at each stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51ef6c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "def ingest(spark, path_a: str, path_b: str) -> Tuple[DataFrame, DataFrame]:\n",
    "    \"\"\"Load SOURCE_A and SOURCE_B. Apply explicit schemas where possible.\n",
    "    Return two DataFrames with uniform column naming.\n",
    "    \"\"\"\n",
    "    # write some code here\n",
    "    df_a = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(path_a)\n",
    "    df_b = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(path_b)\n",
    "\n",
    "    print(f\"Ingested A: {df_a.count()} rows\")\n",
    "    print(f\"Ingested B: {df_b.count()} rows\")\n",
    "\n",
    "    return df_a, df_b\n",
    "\n",
    "def transform(df_a: DataFrame, df_b: DataFrame) -> DataFrame:\n",
    "    \"\"\"Clean, deduplicate, and normalize. Add parsed timestamps.\n",
    "    Drop obvious null records. Prepare keys for join.\n",
    "    \"\"\"\n",
    "    # write some code here\n",
    "    df_clean = df_a.dropDuplicates()\n",
    "    \n",
    "    if \"event_time\" in df_clean.columns:\n",
    "        df_clean = df_clean.withColumn(\"event_time\", F.to_timestamp(\"event_time\"))\n",
    "        df_clean = df_clean.withColumn(\"date\", F.to_date(\"event_time\"))\n",
    "    \n",
    "    required_cols = [c for c in [\"session_id\", \"product_id\"] if c in df_clean.columns]\n",
    "    df_clean = df_clean.dropna(subset=required_cols)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def join_and_aggregate(df: DataFrame, dim: DataFrame) -> DataFrame:\n",
    "    \"\"\"Join with dim table. Handle potential skew (hint: salting or AQE).\n",
    "    Compute business aggregates with window or groupBy.\n",
    "    \"\"\"\n",
    "    # write some code here\n",
    "    common_cols = list(set(df.columns) & set(dim.columns))\n",
    "    \n",
    "    if not common_cols:\n",
    "        print(\"Warning: No common columns found for join.\")\n",
    "        return df\n",
    "        \n",
    "    join_col = common_cols[0] \n",
    "    \n",
    "    df_joined = df.join(F.broadcast(dim), on=join_col, how=\"inner\")\n",
    "    \n",
    "    if \"brand\" in df_joined.columns and \"price\" in df_joined.columns and \"date\" in df_joined.columns:\n",
    "        res = (df_joined\n",
    "               .groupBy(\"date\", \"brand\")\n",
    "               .agg(\n",
    "                   F.count(\"*\").alias(\"total_events\"),\n",
    "                   F.sum(\"price\").alias(\"total_sales\")\n",
    "               )\n",
    "               .orderBy(\"date\", F.desc(\"total_sales\"))\n",
    "              )\n",
    "        return res\n",
    "    \n",
    "    return df_joined\n",
    "\n",
    "def write_out(df: DataFrame, base: str, partitions: list[str]) -> None:\n",
    "    \"\"\"Write Parquet, overwrite mode, partitioned by `partitions`.\n",
    "    Optimize small files if needed (coalesce).\n",
    "    \"\"\"\n",
    "    # write some code here\n",
    "    writer = df.coalesce(1).write.mode(\"overwrite\")\n",
    "    \n",
    "    valid_partitions = [p for p in partitions if p in df.columns]\n",
    "    if valid_partitions:\n",
    "        writer = writer.partitionBy(*valid_partitions)\n",
    "        \n",
    "    writer.parquet(base)\n",
    "    print(f\"Data written to {base} partitioned by {valid_partitions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bdc908",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "1. **Ingest**: read `SOURCE_A_PATH`, `SOURCE_B_PATH`. Provide explicit schemas. Count rows and malformed records.\n",
    "2. **Transform**: standardize column names, cast types, parse timestamps into UTC, deduplicate using keys.\n",
    "3. **Join + Aggregate**: explain your join strategy. Mitigate skew. Produce a tidy table with daily metrics.\n",
    "4. **Store**: write partitioned Parquet to `OUTPUT_BASE`, e.g., partition by `date` and one categorical column.\n",
    "5. **Explain plans**: capture `df.explain(mode='formatted')` for transform, join, and final write.\n",
    "6. **Quality gates**: implement three checks (row count non‑zero, null rate thresholds, referential coverage). Abort if a gate fails.\n",
    "7. **Reproducibility**: document your Spark config and any seeds. Describe how to re‑run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7af6b4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Pipeline execution...\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1316.csv.\n: org.apache.spark.SparkException: [INTERNAL_ERROR] The \"head\" action failed. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace. SQLSTATE: XX000\n\tat org.apache.spark.SparkException$.internalError(SparkException.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:643)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:656)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:121)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:72)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:63)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:219)\n\tat scala.Option.orElse(Option.scala:477)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:216)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:422)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)\n\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:259)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.spark.SparkException$.internalError(SparkException.scala:107)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:643)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:656)\n\t\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n\t\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\t\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n\t\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n\t\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:121)\n\t\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:72)\n\t\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:63)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:219)\n\t\tat scala.Option.orElse(Option.scala:477)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:216)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:422)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\t\tat scala.Option.getOrElse(Option.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 22 more\nCaused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.sql.classic.SparkSession.sparkContext()\" because the return value of \"org.apache.spark.sql.execution.SparkPlan.session()\" is null\n\tat org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:68)\n\tat org.apache.spark.sql.execution.CollectLimitExec.readMetrics$lzycompute(limit.scala:68)\n\tat org.apache.spark.sql.execution.CollectLimitExec.readMetrics(limit.scala:67)\n\tat org.apache.spark.sql.execution.CollectLimitExec.metrics$lzycompute(limit.scala:69)\n\tat org.apache.spark.sql.execution.CollectLimitExec.metrics(limit.scala:69)\n\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:147)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2233)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:121)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:72)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:63)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:219)\n\tat scala.Option.orElse(Option.scala:477)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:216)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:422)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t... 22 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m SOURCE_A_PATH \u001b[38;5;129;01mand\u001b[39;00m SOURCE_B_PATH:\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting Pipeline execution...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     df_a, df_b = \u001b[43mingest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSOURCE_A_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSOURCE_B_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     stg = transform(df_a, df_b)\n\u001b[32m     21\u001b[39m     out = join_and_aggregate(stg, df_b)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mingest\u001b[39m\u001b[34m(spark, path_a, path_b)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load SOURCE_A and SOURCE_B. Apply explicit schemas where possible.\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03mReturn two DataFrames with uniform column naming.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# write some code here\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m df_a = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mheader\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minferSchema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m df_b = spark.read.option(\u001b[33m\"\u001b[39m\u001b[33mheader\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m).option(\u001b[33m\"\u001b[39m\u001b[33minferSchema\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m).csv(path_b)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIngested A: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_a.count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/de1-env/lib/python3.11/site-packages/pyspark/sql/readwriter.py:838\u001b[39m, in \u001b[36mDataFrameReader.csv\u001b[39m\u001b[34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[39m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) == \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark._sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_only():\n\u001b[32m    841\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrdd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RDD  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/de1-env/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/de1-env/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/de1-env/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o1316.csv.\n: org.apache.spark.SparkException: [INTERNAL_ERROR] The \"head\" action failed. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace. SQLSTATE: XX000\n\tat org.apache.spark.SparkException$.internalError(SparkException.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:643)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:656)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:121)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:72)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:63)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:219)\n\tat scala.Option.orElse(Option.scala:477)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:216)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:422)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)\n\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:259)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.spark.SparkException$.internalError(SparkException.scala:107)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:643)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:656)\n\t\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n\t\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\t\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n\t\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n\t\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:121)\n\t\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:72)\n\t\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:63)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:219)\n\t\tat scala.Option.orElse(Option.scala:477)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:216)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:422)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\t\tat scala.Option.getOrElse(Option.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 22 more\nCaused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.sql.classic.SparkSession.sparkContext()\" because the return value of \"org.apache.spark.sql.execution.SparkPlan.session()\" is null\n\tat org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:68)\n\tat org.apache.spark.sql.execution.CollectLimitExec.readMetrics$lzycompute(limit.scala:68)\n\tat org.apache.spark.sql.execution.CollectLimitExec.readMetrics(limit.scala:67)\n\tat org.apache.spark.sql.execution.CollectLimitExec.metrics$lzycompute(limit.scala:69)\n\tat org.apache.spark.sql.execution.CollectLimitExec.metrics(limit.scala:69)\n\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:147)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2233)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:121)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:72)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:63)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:219)\n\tat scala.Option.orElse(Option.scala:477)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:216)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:422)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t... 22 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "# Orchestration\n",
    "## Replace raises with your implementation, then run this driver.\n",
    "if spark is not None:\n",
    "    # write some code here\n",
    "    # df_a, df_b = ingest(spark, SOURCE_A_PATH, SOURCE_B_PATH)\n",
    "    # stg = transform(df_a, df_b)\n",
    "    # out = join_and_aggregate(stg, df_b)\n",
    "    # print('Final count:', out.count())\n",
    "    # print('Plan:')\n",
    "    # out.explain(mode='formatted')\n",
    "\n",
    "    if SOURCE_A_PATH and SOURCE_B_PATH:\n",
    "        print(\"Starting Pipeline execution...\")\n",
    "        \n",
    "        df_a, df_b = ingest(spark, SOURCE_A_PATH, SOURCE_B_PATH)\n",
    "        \n",
    "        stg = transform(df_a, df_b)\n",
    "        \n",
    "        out = join_and_aggregate(stg, df_b)\n",
    "        \n",
    "        print('Final count (Aggregated rows):', out.count())\n",
    "        print('Plan:')\n",
    "        out.explain(mode='formatted')\n",
    "        \n",
    "        write_out(out, OUTPUT_BASE, partitions=[\"date\"])\n",
    "    \n",
    "else:\n",
    "    print(\"Les fichiers sources n'existent pas encore.\")\n",
    "    print(\"Exécuter la section '3. The Extract in ETL' plus bas dans le notebook\")\n",
    "    print(\"pour extraire les CSV depuis PostgreSQL, puis revenir exécuter cette cellule.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78262290",
   "metadata": {},
   "source": [
    "# Assignment 2: ETL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e1996-383c-4a4a-b3da-30edcd6e1762",
   "metadata": {},
   "source": [
    "## 1. Querying the Operational Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ab400",
   "metadata": {},
   "source": [
    "Let's run a query to verify that the operational database has been properly restored and that we can issue a query to PostgreSQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4d7786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping `postgresql@17`... (might take a while)\n",
      "\u001b[34m==>\u001b[0m \u001b[1mSuccessfully stopped `postgresql@17` (label: homebrew.mxcl.postgresql@17)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!brew services stop postgresql@17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1696be08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m==>\u001b[0m \u001b[1mSuccessfully started `postgresql@17` (label: homebrew.mxcl.postgresql@17)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!brew services start postgresql@17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9bc0b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/homebrew/opt/postgresql@17/bin/dropdb --if-exists esiee_full\n",
    "!/opt/homebrew/opt/postgresql@17/bin/createdb esiee_full\n",
    "!/opt/homebrew/opt/postgresql@17/bin/pg_restore   -d esiee_full   --jobs 4 --clean --if-exists   --no-owner --no-acl   retail_schema_20250826.dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a33f61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE ROLE\n",
      "GRANT\n",
      "GRANT\n",
      "GRANT\n",
      "ALTER DEFAULT PRIVILEGES\n"
     ]
    }
   ],
   "source": [
    "# First cell won't run\n",
    "# !PGPORT=5433 psql \"esiee_full\" -v ON_ERROR_STOP=1 -c \"SELECT COUNT(DISTINCT user_id) AS number_users FROM retail.user;\"\n",
    "\n",
    "# Solution 1\n",
    "# !sudo -u postgres env PGPORT=5433 psql \"esiee_full\" -v ON_ERROR_STOP=1 -c \"SELECT COUNT(DISTINCT user_id) AS number_users FROM retail.user;\"\n",
    "\n",
    "# Solution 2\n",
    "# 1- Run this in your terminal to create a read-only user for the database :\n",
    "!/opt/homebrew/opt/postgresql@17/bin/psql -d esiee_full -p 5432 -c \"CREATE ROLE esiee_reader LOGIN PASSWORD 'azerty123'; GRANT CONNECT ON DATABASE esiee_full TO esiee_reader; GRANT USAGE ON SCHEMA retail TO esiee_reader; GRANT SELECT ON ALL TABLES IN SCHEMA retail TO esiee_reader; ALTER DEFAULT PRIVILEGES IN SCHEMA retail GRANT SELECT ON TABLES TO esiee_reader;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e21a8d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2- Then, in your notebook, set the environment variables like this:\n",
    "import os\n",
    "os.environ['PGHOST'] = '127.0.0.1'   # force TCP (not Unix socket)\n",
    "os.environ['PGPORT'] = '5432'\n",
    "os.environ['PGUSER'] = 'esiee_reader'\n",
    "os.environ['PGPASSWORD'] = 'azerty123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "51485da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " number_users \n",
      "--------------\n",
      "      3022290\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3- then\n",
    "!psql -d esiee_full -c 'SELECT COUNT(DISTINCT user_id) AS number_users FROM retail.\"user\";'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851c65a-e658-4044-8878-4c3d3722463e",
   "metadata": {},
   "source": [
    "**The correct answer should be 3022290.**\n",
    "\n",
    "If running the cell above gives you the same answer, the everything should be in order.\n",
    "\n",
    "If you're getting an error, fix it before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ecb7e53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  count   \n",
      "----------\n",
      " 42418541\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!psql -d esiee_full -c \"select count(*) from retail.events\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "340bac35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            List of relations\n",
      " Schema |     Name     | Type  |  Owner  \n",
      "--------+--------------+-------+---------\n",
      " retail | brand        | table | lorenzo\n",
      " retail | category     | table | lorenzo\n",
      " retail | events       | table | lorenzo\n",
      " retail | product      | table | lorenzo\n",
      " retail | product_name | table | lorenzo\n",
      " retail | session      | table | lorenzo\n",
      " retail | user         | table | lorenzo\n",
      "(7 rows)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!psql -d esiee_full -c \"\\dt retail.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bf3ece87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       event_time       | event_type |              session_id              | product_id |  price   \n",
      "------------------------+------------+--------------------------------------+------------+----------\n",
      " 2019-10-01 04:17:42+02 | view       | 521147bb-459e-4874-8153-3ab2fa290ad3 |   13700159 |   470.85\n",
      " 2019-10-01 04:17:42+02 | view       | 34253b2e-b8eb-4e0a-b758-f376a41e96df |    4803710 |    25.71\n",
      " 2019-10-01 04:17:42+02 | view       | dfc31a56-7292-4f70-b098-d0a2812fbdc6 |   13700226 | 18173.00\n",
      " 2019-10-01 04:17:42+02 | view       | 4d74b49e-3838-43cc-ac2e-a0490942b464 |   13103842 |   391.26\n",
      " 2019-10-01 04:17:42+02 | view       | 92d24e81-856a-4157-8689-d38a4a8210bf |    1004669 |    74.39\n",
      " 2019-10-01 04:17:43+02 | view       | d5de76ec-7ef2-4d78-bf16-220f2ca3fcdb |    1004247 |   809.72\n",
      " 2019-10-01 04:17:43+02 | view       | fd492d02-c6d6-4575-a967-e7136853eb0a |   18000964 |    19.30\n",
      " 2019-10-01 04:17:43+02 | view       | d4fbe676-9a7e-48d0-9c38-a8e54a7408f5 |    9101345 |    72.14\n",
      " 2019-10-01 04:17:44+02 | view       | bc9ffbe5-2245-4668-add5-3238838db13f |   16600063 |   395.89\n",
      " 2019-10-01 04:17:44+02 | view       | 5271b506-9576-438f-b8c7-a7eefa8c6dba |    3100541 |    36.01\n",
      "(10 rows)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!psql -d esiee_full -c \"select * from retail.events limit 10\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d489b61-7304-4513-8c79-89c07745a3ec",
   "metadata": {},
   "source": [
    "As a warmup exercise, write SQL queries against the operational database to answer the following questions and report the answers.\n",
    "Place both your SQL queries and answers in the following cell, replacing the placeholder texts that exist there.\n",
    "Each question needs to be answered by a _single_ SQL query (that is, it is not acceptable to run multiple SQL queries and then compute the answer yourself).\n",
    "\n",
    "1. For `session_id` `789d3699-028e-4367-b515-b82e2cb5225f`, what was the purchase price?\n",
    "2. How many products are sold by the brand \"sokolov\"?\n",
    "3. What is the average purchase price of items purchased from the brand \"febest\"?\n",
    "4. What is average number of events per user? (Report answer to two digits after the decimal point, i.e., XX.XX)\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b515132-07e2-44e3-abea-287b0a89af4a",
   "metadata": {},
   "source": [
    "// qcell_1b76x2 (keep this id for tracking purposes)\n",
    "\n",
    "**Q1 SQL:**\n",
    "\n",
    "SELECT price\n",
    "FROM retail.events \n",
    "WHERE session_id = '789d3699-028e-4367-b515-b82e2cb5225f' \n",
    "AND event_type = 'purchase';\n",
    "\n",
    "**Q1 answer:**\n",
    "\n",
    " 100.39\n",
    "\n",
    "**Q2 SQL:**\n",
    "\n",
    "SELECT COUNT(DISTINCT e.product_id) \n",
    "FROM retail.events e\n",
    "JOIN retail.product p ON e.product_id= p.product_id\n",
    "WHERE brand = 'sokolov';\n",
    "\n",
    "**Q2 answer:**\n",
    "\n",
    "1601\n",
    "\n",
    "**Q3 SQL:**\n",
    "\n",
    "SELECT AVG(e.price) \n",
    "FROM retail.events e\n",
    "JOIN retail.product p ON e.product_id= p.product_id\n",
    "WHERE p.brand = 'febest' \n",
    "AND e.event_type = 'purchase';\n",
    "\n",
    "**Q3 answer:**\n",
    "\n",
    "20.3934782608695652\n",
    "\n",
    "**Q4 SQL:**\n",
    "\n",
    "SELECT ROUND(COUNT(*)::numeric / COUNT(DISTINCT session_id), 2) \n",
    "FROM retail.events;  \n",
    "\n",
    "**Q4 answer:**\n",
    "\n",
    "4.59\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8f486e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " price  \n",
      "--------\n",
      " 100.39\n",
      "(1 row)\n",
      "\n",
      "       event_time       | event_type |              session_id              | product_id | price  \n",
      "------------------------+------------+--------------------------------------+------------+--------\n",
      " 2019-10-01 04:46:44+02 | purchase   | 789d3699-028e-4367-b515-b82e2cb5225f |   28401176 | 100.39\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q1 answer :\n",
    "\n",
    "!psql \"esiee_full\" -v ON_ERROR_STOP=1 -c \"SELECT price \\\n",
    "FROM retail.events \\\n",
    "WHERE session_id = '789d3699-028e-4367-b515-b82e2cb5225f' \\\n",
    "AND event_type = 'purchase';\"\n",
    "\n",
    "# or\n",
    "\n",
    "!psql \"esiee_full\" -v ON_ERROR_STOP=1 -c \"SELECT * \\\n",
    "FROM retail.events \\\n",
    "WHERE session_id = '789d3699-028e-4367-b515-b82e2cb5225f' \\\n",
    "AND event_type = 'purchase';\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ae006cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " count \n",
      "-------\n",
      "  1601\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q2 answer :\n",
    "\n",
    "!psql \"esiee_full\" -v ON_ERROR_STOP=1 -c \"SELECT COUNT(DISTINCT e.product_id) \\\n",
    "FROM retail.events e\\\n",
    "JOIN retail.product p ON e.product_id= p.product_id\\\n",
    "WHERE brand = 'sokolov' \\\n",
    ";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8ac885b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         avg         \n",
      "---------------------\n",
      " 20.3934782608695652\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q3 answer :\n",
    "\n",
    "!psql \"esiee_full\" -v ON_ERROR_STOP=1 -c \"SELECT AVG(e.price) \\\n",
    "FROM retail.events e\\\n",
    "JOIN retail.product p ON e.product_id= p.product_id\\\n",
    "WHERE p.brand = 'febest' \\\n",
    "AND e.event_type = 'purchase' \\\n",
    ";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "518f6baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " round \n",
      "-------\n",
      "  4.59\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q4 answer :\n",
    "\n",
    "!psql \"esiee_full\" -v ON_ERROR_STOP=1 -c \"SELECT ROUND(COUNT(*)::numeric / COUNT(DISTINCT session_id), 2) \\\n",
    "FROM retail.events\\\n",
    ";\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fecda4-fc6e-4767-af8f-c08d72eeaa02",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec996e",
   "metadata": {},
   "source": [
    "The following cell contains setup to measure wall clock time and memory usage. (Don't worry about the details, just run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39cbf5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psutil is installed.\n"
     ]
    }
   ],
   "source": [
    "# !brew install numpy pandas pyarrow matplotlib scipy\n",
    "import sys, subprocess\n",
    "try:\n",
    "    import psutil  # noqa: F401\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"psutil\"])\n",
    "print(\"psutil is installed.\")\n",
    "\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "import time, os, platform\n",
    "\n",
    "# Try to import optional modules\n",
    "try:\n",
    "    import psutil\n",
    "except Exception:\n",
    "    psutil = None\n",
    "\n",
    "try:\n",
    "    import resource  # not available on Windows\n",
    "except Exception:\n",
    "    resource = None\n",
    "\n",
    "\n",
    "def _rss_bytes():\n",
    "    \"\"\"Resident Set Size in bytes (cross-platform via psutil if available).\"\"\"\n",
    "    if psutil is not None:\n",
    "        return psutil.Process(os.getpid()).memory_info().rss\n",
    "    # Fallback: unknown RSS → 0 \n",
    "    return 0\n",
    "\n",
    "\n",
    "def _peak_bytes():\n",
    "    \"\"\"\n",
    "    Best-effort peak memory in bytes.\n",
    "    - Windows: psutil peak working set (peak_wset)\n",
    "    - Linux:   resource.ru_maxrss (KB → bytes)\n",
    "    - macOS:   resource.ru_maxrss (bytes)\n",
    "    Fallback to current RSS if unavailable.\n",
    "    \"\"\"\n",
    "    sysname = platform.system()\n",
    "\n",
    "    # Windows path: use psutil peak_wset if present\n",
    "    if sysname == \"Windows\" and psutil is not None:\n",
    "        mi = psutil.Process(os.getpid()).memory_info()\n",
    "        peak = getattr(mi, \"peak_wset\", None)  # should be available on Windows\n",
    "        if peak is not None:\n",
    "            return int(peak)\n",
    "        return int(mi.rss)\n",
    "\n",
    "    # POSIX path: resource may be available\n",
    "    if resource is not None:\n",
    "        try:\n",
    "            ru = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "            # On Linux ru_maxrss is in kilobytes; on macOS/BSD it is bytes\n",
    "            if sysname == \"Linux\":\n",
    "                return int(ru) * 1024\n",
    "            else:\n",
    "                return int(ru)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Last resort\n",
    "    return _rss_bytes()\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def timemem(line, cell):\n",
    "    \"\"\"\n",
    "    Measure wall time and memory around the execution of this cell.\n",
    "\n",
    "        %%timemem\n",
    "        <your code>\n",
    "\n",
    "    Notes:\n",
    "    - RSS = resident memory after the cell.\n",
    "    - Peak is OS-dependent (see _peak_bytes docstring).\n",
    "    \"\"\"\n",
    "    ip = get_ipython()\n",
    "\n",
    "    rss_before  = _rss_bytes()\n",
    "    peak_before = _peak_bytes()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Execute the cell body\n",
    "    result = ip.run_cell(cell)\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    rss_after  = _rss_bytes()\n",
    "    peak_after = _peak_bytes()\n",
    "\n",
    "    wall = t1 - t0\n",
    "    rss_delta_mb  = (rss_after  - rss_before)  / (1024 * 1024)\n",
    "    peak_delta_mb = (peak_after - peak_before) / (1024 * 1024)\n",
    "\n",
    "    print(\"======================================\")\n",
    "    print(f\"Wall time: {wall:.3f} s\")\n",
    "    print(f\"RSS Δ: {rss_delta_mb:+.2f} MB\")\n",
    "    print(f\"Peak memory Δ: {peak_delta_mb:+.2f} MB (OS-dependent)\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620ca1f2-5c25-49df-9570-f5e56782b55c",
   "metadata": {},
   "source": [
    "## 3. The \"Extract\" in ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb0763",
   "metadata": {},
   "source": [
    "The operational database comprises the tables described in the helper.\n",
    "\n",
    "For the \"Extract\" in ETL, we're going to extract the following CSV files, each corresponding to a table in the operational database:\n",
    "\n",
    "- **user.csv**: `user_id, gender, birthdate`\n",
    "- **session.csv**: `session_id, user_id`\n",
    "- **product.csv**: `product_id, brand, category, product_name`\n",
    "- **product_name.csv**: `category, product_name, description`\n",
    "- **events.csv**: `event_time, event_type, session_id, product_id, price`\n",
    "- **category.csv**: `category, description`\n",
    "- **brand.csv**: `brand, description`\n",
    "\n",
    "From these files, we'll build a data warehouse organized in a standard star schema that has the following tables:\n",
    "\n",
    "- Dimension tables: `dim_user`, `dim_age`, `dim_brand`, `dim_category`, `dim_product`, `dim_date`, `dim_session`\n",
    "- The main fact table `fact_events` with foreign keys: `date_key, user_key, age_key, product_key, brand_key, category_key, session_key`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d94b96-231a-4db7-9c60-5ae521a38aae",
   "metadata": {},
   "source": [
    "Let's specify a \"base directory\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd673d3-8c30-489a-822c-27419c1e1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to path on your local machine.\n",
    "BASE_DIR = \"/Users/lorenzo/Documents/Cours/E4FD/Data-Engineering/Lab2/lab2Assignment\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25b07b-0a28-499a-9817-0630640aea20",
   "metadata": {},
   "source": [
    "These are the commands that perform the \"extraction\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f430b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPY 3022290\n",
      "COPY 9244421\n",
      "COPY 13\n",
      "COPY 3444\n",
      "COPY 127\n",
      "COPY 166794\n",
      "COPY 42418541\n"
     ]
    }
   ],
   "source": [
    "!psql esiee_full -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"user\"         TO '\\''{BASE_DIR}/user.csv'\\''         WITH (FORMAT csv, HEADER true)'\n",
    "!psql esiee_full -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"session\"      TO '\\''{BASE_DIR}/session.csv'\\''      WITH (FORMAT csv, HEADER true)'\n",
    "!psql esiee_full -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"category\"     TO '\\''{BASE_DIR}/category.csv'\\''     WITH (FORMAT csv, HEADER true)'\n",
    "!psql esiee_full -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"brand\"        TO '\\''{BASE_DIR}/brand.csv'\\''        WITH (FORMAT csv, HEADER true)'\n",
    "!psql esiee_full -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"product_name\" TO '\\''{BASE_DIR}/product_name.csv'\\'' WITH (FORMAT csv, HEADER true)'\n",
    "!psql esiee_full -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"product\"      TO '\\''{BASE_DIR}/product.csv'\\''      WITH (FORMAT csv, HEADER true)'\n",
    "!psql esiee_full -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"events\"       TO '\\''{BASE_DIR}/events.csv'\\''       WITH (FORMAT csv, HEADER true)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f0dde-6c17-48ad-aa4e-6e4019b106bc",
   "metadata": {},
   "source": [
    "(Note that the quote style above will _not_ work for Windows machines. Please adjust accordingly.)\n",
    "\n",
    "After the extraction, you should have 7 CSV files, each corresponding to a table in the operational database.\n",
    "\n",
    "The CSV files should be stored in `BASE_DIR`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930cdf5-b989-4608-8a03-22c0eab6b031",
   "metadata": {},
   "source": [
    "The following code snippet should \"just work\" to initialize Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe303ff-e323-4389-a64f-d9e730fa1ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/29 15:36:59 WARN Utils: Your hostname, MacBook-Air-de-Lorenzo.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.58 instead (on interface en0)\n",
      "25/12/29 15:36:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/29 15:37:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.58:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>A2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x104c9a710>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark, os, sys\n",
    "\n",
    "# Change to path on your local machine.\n",
    "os.environ[\"SPARK_HOME\"] = \"/Users/lorenzo/miniconda3/envs/de1-env/lib/python3.11/site-packages/pyspark\"\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "py = sys.executable  # the Python of this notebook (e.g., .../envs/yourenv/bin/python)\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = py\n",
    "os.environ[\"PYSPARK_PYTHON\"] = py\n",
    "\n",
    "spark = SparkSession.getActiveSession() or (\n",
    "    SparkSession.builder\n",
    "    .appName(\"A2\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")           # or 12g+\n",
    "    .config(\"spark.sql.shuffle.partitions\",\"400\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.pyspark.driver.python\", py)\n",
    "    .config(\"spark.pyspark.python\", py)\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\", py)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec616b-b333-4eb6-8d7e-ab002af8f668",
   "metadata": {},
   "source": [
    "At this point, Spark should be initialized.\n",
    "\n",
    "Let's then load in CSV files into DataFrames.\n",
    "\n",
    "write some code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49534dee-f56a-4f6b-8930-64092a0fcba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 3022290\n",
      "session: 9244421\n",
      "product: 166794\n",
      "product_name: 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events: 42418541\n",
      "category: 13\n",
      "brand: 3444\n",
      "======================================\n",
      "Wall time: 18.115 s\n",
      "RSS Δ: +0.27 MB\n",
      "Peak memory Δ: +0.27 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 1070d0410, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 1070d0510, raw_cell=\"# codecell_30z8le (keep this id for tracking purpo..\" transformed_cell=\"# codecell_30z8le (keep this id for tracking purpo..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_30z8le (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "# Chargement des DataFrames\n",
    "df_user = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{BASE_DIR}/user.csv\")\n",
    "df_session = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{BASE_DIR}/session.csv\")\n",
    "df_product = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{BASE_DIR}/product.csv\")\n",
    "df_product_name = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{BASE_DIR}/product_name.csv\")\n",
    "df_events = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{BASE_DIR}/events.csv\")\n",
    "df_category = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{BASE_DIR}/category.csv\")\n",
    "df_brand = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{BASE_DIR}/brand.csv\")\n",
    "\n",
    "# By the time we get to here, we've loaded each of the CSV files into a corresponding dataframe.\n",
    "# Let's count the number of records in each:\n",
    "\n",
    "print(f\"user: {df_user.count()}\")\n",
    "print(f\"session: {df_session.count()}\")\n",
    "print(f\"product: {df_product.count()}\")\n",
    "print(f\"product_name: {df_product_name.count()}\")\n",
    "print(f\"events: {df_events.count()}\")\n",
    "print(f\"category: {df_category.count()}\")\n",
    "print(f\"brand: {df_brand.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4552f156-6f37-4938-8e93-1a02a38ba760",
   "metadata": {},
   "source": [
    "How do you know if you've done everything correctly?\n",
    "\n",
    "Well, issue the SQL query `select count(*) from retail.user;` to count the number of rows in the `user` table in the operational database.\n",
    "It should match the output of `df_user.count()`; same for the other tables.\n",
    "If the counts match, then you know everything is in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a2b6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " table_name |  count  \n",
      "------------+---------\n",
      " user       | 3022290\n",
      "(1 row)\n",
      "\n",
      " table_name |  count  \n",
      "------------+---------\n",
      " session    | 9244421\n",
      "(1 row)\n",
      "\n",
      " table_name | count  \n",
      "------------+--------\n",
      " product    | 166794\n",
      "(1 row)\n",
      "\n",
      "  table_name  | count \n",
      "--------------+-------\n",
      " product_name |   127\n",
      "(1 row)\n",
      "\n",
      " table_name |  count   \n",
      "------------+----------\n",
      " events     | 42418541\n",
      "(1 row)\n",
      "\n",
      " table_name | count \n",
      "------------+-------\n",
      " category   |    13\n",
      "(1 row)\n",
      "\n",
      " table_name | count \n",
      "------------+-------\n",
      " brand      |  3444\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Ce script interroge la base de données pour compter les lignes de chaque table et vérifier si elles correspondent aux DataFrames chargés.\n",
    "psql esiee_full -c \"SELECT 'user' as table_name, count(*) FROM retail.user;\"\n",
    "psql esiee_full -c \"SELECT 'session' as table_name, count(*) FROM retail.session;\"\n",
    "psql esiee_full -c \"SELECT 'product' as table_name, count(*) FROM retail.product;\"\n",
    "psql esiee_full -c \"SELECT 'product_name' as table_name, count(*) FROM retail.product_name;\"\n",
    "psql esiee_full -c \"SELECT 'events' as table_name, count(*) FROM retail.events;\"\n",
    "psql esiee_full -c \"SELECT 'category' as table_name, count(*) FROM retail.category;\"\n",
    "psql esiee_full -c \"SELECT 'brand' as table_name, count(*) FROM retail.brand;\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e10f63",
   "metadata": {},
   "source": [
    "## 4. Build the Dimensions Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3303f4-e9be-455b-a458-91de0ebdded7",
   "metadata": {},
   "source": [
    "### 4.1 The `user` Dimension Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f4ee83",
   "metadata": {},
   "source": [
    "Build the `dim_user` dimension table.\n",
    "This table should include `user_key`, `user_id`, `gender`, `birthdate`, and `generation`. \n",
    "\n",
    "Set `generation` to one of the following values based on the birth year: \n",
    "- \"Traditionalists\": born 1925 to 1945\n",
    "- \"Boomers\": born 1946 to 1964\n",
    "- \"GenX\": born 1965 to 1980\n",
    "- \"Millennials\": born 1981 to 2000\n",
    "- \"GenZ\": born 2001 to 2020\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "883cc05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3022290"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Wall time: 0.235 s\n",
      "RSS Δ: +0.06 MB\n",
      "Peak memory Δ: +0.05 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 103fdd0d0, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 103fdf810, raw_cell=\"# codecell_41ax14 (keep this id for tracking purpo..\" transformed_cell=\"# codecell_41ax14 (keep this id for tracking purpo..\" store_history=False silent=False shell_futures=True cell_id=None> result=3022290>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_41ax14 (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "from pyspark.sql.functions import col, year, when, monotonically_increasing_id\n",
    "\n",
    "# 1. Définition de la logique pour les générations\n",
    "generation_expr = (\n",
    "    when((year(col(\"birthdate\")).between(1925, 1945)), \"Traditionalists\")\n",
    "    .when((year(col(\"birthdate\")).between(1946, 1964)), \"Boomers\")\n",
    "    .when((year(col(\"birthdate\")).between(1965, 1980)), \"GenX\")\n",
    "    .when((year(col(\"birthdate\")).between(1981, 2000)), \"Millennials\")\n",
    "    .when((year(col(\"birthdate\")).between(2001, 2020)), \"GenZ\")\n",
    "    .otherwise(\"Unknown\") \n",
    ")\n",
    "\n",
    "# 2. Construction du DataFrame final\n",
    "dim_user = df_user.withColumn(\"generation\", generation_expr) \\\n",
    "                  .withColumn(\"user_key\", monotonically_increasing_id()) \\\n",
    "                  .select(\"user_key\", \"user_id\", \"gender\", \"birthdate\", \"generation\")\n",
    "# By the time we get to here, \"dim_user\" should hold the user dimensions table according to the specification above.\n",
    "\n",
    "dim_user.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6e753-8515-4924-b4d7-f4a8eeb3c4f8",
   "metadata": {},
   "source": [
    "**The correct answer should be 3022290.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eadced-3481-480a-9320-a07bbfc6d6b7",
   "metadata": {},
   "source": [
    "### 4.2 The `age` Dimension Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6758f1e8",
   "metadata": {},
   "source": [
    "Even though `birthdate` exists in `dim_user`, a separate `dim_age` is helpful because it:\n",
    "- Simplifies analysis with ready-made bands.\n",
    "- Ensures consistency across all queries.\n",
    "- Improves performance via small surrogate keys.\n",
    "- Preserves history by fixing age at event time.\n",
    "- Adds flexibility to adjust bands without changing facts.\n",
    "\n",
    "We're going to build a `dim_age` table that has 4 columns:\n",
    "- `age_key`: (INT, surrogate PK)\n",
    "- `age_band`: (STRING) following the age band rules below\n",
    "- `min_age`: (INT)\n",
    "- `max_age`: (INT)\n",
    "\n",
    "Bands:\n",
    "- \"<18\": min_age = NULL, max_age = 17\n",
    "- \"18-24\": 18, 24\n",
    "- \"25-34\": 25, 34\n",
    "- \"35-44\": 35, 44\n",
    "- \"45-54\": 45, 54\n",
    "- \"55-64\": 55, 64\n",
    "- \"65-74\": 65, 74\n",
    "- \"75-84\": 75, 84\n",
    "- \"85-94\": 85, 94\n",
    "- \"unknown\": NULL, NULL\n",
    "\n",
    "The construction of this table is a bit tricky, so we're going to show you how to do it, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ed16029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Wall time: 0.212 s\n",
      "RSS Δ: +0.00 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 1075d4390, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 1075d4d90, raw_cell=\"from pyspark.sql.window import Window\n",
       "\n",
       "# Static ag..\" transformed_cell=\"from pyspark.sql.window import Window\n",
       "\n",
       "# Static ag..\" store_history=False silent=False shell_futures=True cell_id=None> result=10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Static age bands\n",
    "age_band_rows = [\n",
    "    (\"<18\",   None, 17),\n",
    "    (\"18-24\", 18, 24),\n",
    "    (\"25-34\", 25, 34),\n",
    "    (\"35-44\", 35, 44),\n",
    "    (\"45-54\", 45, 54),\n",
    "    (\"55-64\", 55, 64),\n",
    "    (\"65-74\", 65, 74),\n",
    "    (\"75-84\", 75, 84),\n",
    "    (\"85-94\", 85, 94),\n",
    "    (\"unknown\", None, None),\n",
    "]\n",
    "dim_age = spark.createDataFrame(age_band_rows, [\"age_band\", \"min_age\", \"max_age\"])\n",
    "\n",
    "w_age = Window.orderBy(F.col(\"age_band\"))\n",
    "dim_age = dim_age.withColumn(\"age_key\", F.dense_rank().over(w_age))\n",
    "\n",
    "dim_age.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55857d00-560c-4beb-8d29-8626369a3110",
   "metadata": {},
   "source": [
    "**The correct answer should be 10.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6012e1ca-e6ca-46ae-a873-7f5771cd4310",
   "metadata": {},
   "source": [
    "### 4.3 The `brand`, `product`, and `category` Dimension Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe029f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Build the following dimension tables:\n",
    "\n",
    "**`dim_brand`:**\n",
    "- `brand_key` (INT, surrogate PK)\n",
    "- `brand_code` (STRING) \n",
    "- `brand_desc` (STRING)\n",
    "\n",
    "**`dim_category`:**\n",
    "- `category_key` (INT, surrogate PK)\n",
    "- `category_code` (STRING) \n",
    "- `category_desc` (STRING)\n",
    "\n",
    "**`dim_product`:**\n",
    "- `product_key`  (INT, surrogate PK)\n",
    "- `product_id`   (STRING)\n",
    "- `product_desc` (STRING)\n",
    "- `brand_key`   (INT, FK → `dim_brand`)  \n",
    "- `category_key`(INT, FK → `dim_category`)\n",
    "\n",
    "The Learning goals of `dim_product` is to keep all products in `product`, and add details from `product_names`, then join the results with `brand` and `category` dimension tables.\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0acddfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in dim_brand: 3444\n",
      "Number of rows in dim_category: 13\n",
      "Number of rows in dim_product: 166794\n",
      "======================================\n",
      "Wall time: 0.347 s\n",
      "RSS Δ: +0.11 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 1070c44d0, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 1070c7610, raw_cell=\"# codecell_43k3n9 (keep this id for tracking purpo..\" transformed_cell=\"# codecell_43k3n9 (keep this id for tracking purpo..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_43k3n9 (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "# --- 1. Création de dim_brand ---\n",
    "w_brand = Window.orderBy(\"brand\")\n",
    "dim_brand = df_brand.withColumn(\"brand_key\", F.row_number().over(w_brand)) \\\n",
    "    .select(\n",
    "        F.col(\"brand_key\"),\n",
    "        F.col(\"brand\").alias(\"brand_code\"),\n",
    "        F.col(\"brand\").alias(\"brand_desc\")\n",
    "    )\n",
    "\n",
    "# --- 2. Création de dim_category ---\n",
    "w_cat = Window.orderBy(\"category\")\n",
    "# On garde le dataframe complet temporairement pour générer les clés\n",
    "dim_category_full = df_category.withColumn(\"category_key\", F.row_number().over(w_cat))\n",
    "\n",
    "# La dimension finale propre\n",
    "dim_category = dim_category_full.select(\n",
    "    F.col(\"category_key\"),\n",
    "    F.col(\"category\").alias(\"category_code\"),\n",
    "    F.col(\"description\").alias(\"category_desc\")\n",
    ")\n",
    "\n",
    "# --- 3. Création de dim_product ---\n",
    "\n",
    "# --- PRÉPARATION ---\n",
    "# A. Nettoyage product_name : on renomme category pour éviter conflit\n",
    "df_product_name_clean = df_product_name.withColumnRenamed(\"category\", \"cat_link\")\n",
    "\n",
    "# B. Nettoyage category pour la jointure :\n",
    "# On ne garde QUE ce qui est nécessaire pour la jointure (le nom et la clé)\n",
    "# On ne prend pas 'description' ici pour qu'elle ne rentre pas en conflit avec la description du produit\n",
    "dim_category_for_join = dim_category_full.select(\"category\", \"category_key\")\n",
    "\n",
    "\n",
    "# --- JOINTURES ---\n",
    "# Étape A : jointure sur product_name\n",
    "df_p_enriched = df_product.join(df_product_name_clean, on=\"product_name\", how=\"left\")\n",
    "\n",
    "# Étape B : Joindre avec dim_brand\n",
    "df_p_brand = df_p_enriched.join(\n",
    "    dim_brand, \n",
    "    df_p_enriched[\"brand\"] == dim_brand[\"brand_code\"], \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Étape C : Joindre avec la version allégée de category\n",
    "df_p_final = df_p_brand.join(\n",
    "    dim_category_for_join, \n",
    "    df_p_brand[\"cat_link\"] == dim_category_for_join[\"category\"], \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# --- FINALISATION ---\n",
    "# Étape D : Générer product_key final\n",
    "w_prod = Window.orderBy(\"product_id\")\n",
    "\n",
    "dim_product = df_p_final.withColumn(\"product_key\", F.row_number().over(w_prod)) \\\n",
    "    .select(\n",
    "        F.col(\"product_key\"),\n",
    "        F.col(\"product_id\"),\n",
    "        # Maintenant \"description\" est unique (celle du produit), donc plus d'erreur\n",
    "        F.coalesce(F.col(\"description\"), F.col(\"product_name\")).alias(\"product_desc\"),\n",
    "        F.col(\"brand_key\"),\n",
    "        F.col(\"category_key\")\n",
    "    )\n",
    "\n",
    "# By the time we get to here, \"dim_brand\", \"dim_category\", and \"dim_product\" should hold \n",
    "# the dimension tables according to the specifications above.\n",
    "\n",
    "print(f\"Number of rows in dim_brand: {dim_brand.count()}\")\n",
    "print(f\"Number of rows in dim_category: {dim_category.count()}\")\n",
    "print(f\"Number of rows in dim_product: {dim_product.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b819a9-6b4c-4a1f-82a3-e379d1d6a2ad",
   "metadata": {},
   "source": [
    "**Correct answers:**\n",
    "\n",
    "+ Number of rows in `dim_brand`: 3444\n",
    "+ Number of rows in `dim_category`: 13\n",
    "+ Number of rows in `dim_product`: 166794"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60db6b6a-b5af-46ac-ab1c-161cf50ea75a",
   "metadata": {},
   "source": [
    "### 4.4  The `date` Dimension Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f27d5",
   "metadata": {},
   "source": [
    "This table is expected to have one row per calendar date. \n",
    "\n",
    "**`dim_date`:**\n",
    "- `date_key`     (INT, surrogate PK; format YYYYMMDD)\n",
    "- `date`         (DATE, the actual calendar date)\n",
    "- `day`          (INT, 1–31)\n",
    "- `day_of_week`  (INT, 1=Mon … 7=Sun)\n",
    "- `day_name`     (STRING, e.g., Monday)\n",
    "- `is_weekend`   (BOOLEAN)\n",
    "- `week_of_year` (INT, 1–53, ISO week)\n",
    "- `month`        (INT, 1–12)\n",
    "- `month_name`   (STRING, e.g., January)\n",
    "- `quarter`      (INT, 1–4)\n",
    "- `year`         (INT)\n",
    "\n",
    "\n",
    "There are 2025 years, each with 365 days. Do we need to have a table that big? \n",
    "We can, but we do not have to! \n",
    "\n",
    "Instead, follow these instructions to create only as many rows as we need:\n",
    "\n",
    "1. Determine the date range (from the min and max `event_date` in `df_events`).\n",
    "2. Generate all dates in that range with `F.sequence()`.\n",
    "3. Derive attributes (`day`, `day_of_week`, ...).\n",
    "4. Create `date_key` = `year * 10000 + month * 100 + day` (i.e., YYYYMMDD).\n",
    "5. Assign `date_key` as the surrogate PK.\n",
    "\n",
    "Build the `dim_date` table conforming to the specifications above.\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "417d0554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 120:====================================================>  (25 + 1) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de jours générés : 32\n",
      "+--------+----------+---+-----------+---------+----------+------------+-----+----------+-------+----+\n",
      "|date_key|      date|day|day_of_week| day_name|is_weekend|week_of_year|month|month_name|quarter|year|\n",
      "+--------+----------+---+-----------+---------+----------+------------+-----+----------+-------+----+\n",
      "|20191001|2019-10-01|  1|          2|  Tuesday|     false|          40|   10|   October|      4|2019|\n",
      "|20191002|2019-10-02|  2|          3|Wednesday|     false|          40|   10|   October|      4|2019|\n",
      "|20191003|2019-10-03|  3|          4| Thursday|     false|          40|   10|   October|      4|2019|\n",
      "|20191004|2019-10-04|  4|          5|   Friday|     false|          40|   10|   October|      4|2019|\n",
      "|20191005|2019-10-05|  5|          6| Saturday|      true|          40|   10|   October|      4|2019|\n",
      "+--------+----------+---+-----------+---------+----------+------------+-----+----------+-------+----+\n",
      "only showing top 5 rows\n",
      "======================================\n",
      "Wall time: 8.646 s\n",
      "RSS Δ: +0.12 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 104011450, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 104013ed0, raw_cell=\"# codecell_44qm5c (keep this id for tracking purpo..\" transformed_cell=\"# codecell_44qm5c (keep this id for tracking purpo..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_44qm5c (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "# 1. Déterminer la plage de dates (Min et Max)\n",
    "min_max = df_events.select(\n",
    "    F.min(F.to_date(F.col(\"event_time\"))).alias(\"min_date\"),\n",
    "    F.max(F.to_date(F.col(\"event_time\"))).alias(\"max_date\")\n",
    ").collect()[0]\n",
    "\n",
    "start_date = min_max[\"min_date\"]\n",
    "end_date = min_max[\"max_date\"]\n",
    "\n",
    "# 2. Générer la séquence de dates\n",
    "df_date_sequence = spark.range(1).select(\n",
    "    F.explode(\n",
    "        F.sequence(F.lit(start_date), F.lit(end_date))\n",
    "    ).alias(\"date\")\n",
    ")\n",
    "\n",
    "# 3. Calculer tous les attributs demandés\n",
    "dim_date = df_date_sequence.select(\n",
    "    F.col(\"date\"),\n",
    "    F.year(\"date\").alias(\"year\"),\n",
    "    F.month(\"date\").alias(\"month\"),\n",
    "    F.dayofmonth(\"date\").alias(\"day\")\n",
    ").withColumn(\n",
    "    # Clé technique YYYYMMDD\n",
    "    \"date_key\", \n",
    "    F.col(\"year\") * 10000 + F.col(\"month\") * 100 + F.col(\"day\")\n",
    ").withColumn(\n",
    "    # weekday renvoie 0 (Lun) -> 6 (Dim). On ajoute 1 pour avoir 1 -> 7.\n",
    "    \"day_of_week\", \n",
    "    F.weekday(\"date\") + 1\n",
    ").withColumn(\n",
    "    \"day_name\", \n",
    "    F.date_format(\"date\", \"EEEE\")\n",
    ").withColumn(\n",
    "    # Week-end si Samedi (6) ou Dimanche (7)\n",
    "    \"is_weekend\", \n",
    "    F.col(\"day_of_week\").isin([6, 7])\n",
    ").withColumn(\n",
    "    \"week_of_year\", \n",
    "    F.weekofyear(\"date\")\n",
    ").withColumn(\n",
    "    \"month_name\", \n",
    "    F.date_format(\"date\", \"MMMM\")\n",
    ").withColumn(\n",
    "    \"quarter\", \n",
    "    F.quarter(\"date\")\n",
    ").select(\n",
    "    \"date_key\", \"date\", \"day\", \"day_of_week\", \"day_name\", \n",
    "    \"is_weekend\", \"week_of_year\", \"month\", \"month_name\", \n",
    "    \"quarter\", \"year\"\n",
    ")\n",
    "\n",
    "# By the time we get to here, \"dim_date\" should hold the dates dimension table according to the specification above.\n",
    "# Affichage du résultat\n",
    "print(f\"Nombre de jours générés : {dim_date.count()}\")\n",
    "dim_date.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6191b0ce-16fe-4359-bdc2-b23030c5e4f0",
   "metadata": {},
   "source": [
    "**The correct answer should be 32.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e838229",
   "metadata": {},
   "source": [
    "If you reach here, congratulations!\n",
    "You have created all the dimension tables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6ad489b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_user: 3022290\n",
      "dim_age: 10\n",
      "dim_brand: 3444\n",
      "dim_category: 13\n",
      "dim_product: 166794\n",
      "dim_date: 32\n",
      "======================================\n",
      "Wall time: 0.660 s\n",
      "RSS Δ: +0.00 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 10802c410, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 10802f890, raw_cell=\"\n",
       "print(f\"dim_user: {dim_user.count()}\")\n",
       "print(f\"di..\" transformed_cell=\"print(f\"dim_user: {dim_user.count()}\")\n",
       "print(f\"dim..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "\n",
    "print(f\"dim_user: {dim_user.count()}\")\n",
    "print(f\"dim_age: {dim_age.count()}\")\n",
    "print(f\"dim_brand: {dim_brand.count()}\")\n",
    "print(f\"dim_category: {dim_category.count()}\")\n",
    "print(f\"dim_product: {dim_product.count()}\")\n",
    "print(f\"dim_date: {dim_date.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c991409d-c410-46b3-bf2c-b3acb4b5fb28",
   "metadata": {},
   "source": [
    "**Correct answers:**\n",
    "\n",
    "- `dim_user`: 3022290\n",
    "- `dim_age`: 10\n",
    "- `dim_brand`: 3444\n",
    "- `dim_category`: 13\n",
    "- `dim_product`: 166794\n",
    "- `dim_date`: 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1fa63f-101b-444d-9013-20c2947657b4",
   "metadata": {},
   "source": [
    "## 5. Build the Fact Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3523d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Now it's time to build the fact table!\n",
    "\n",
    "Our goal in this step is to create a clean `fact_events` table that joins the events from the operational database to the dimension tables you've just built above.\n",
    "Along the way, we're going to enforce data quality and do a bit of data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb231a1-f368-40b1-aa3c-e448c151bbec",
   "metadata": {},
   "source": [
    "### 5.1 Clean Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75d9cc-9eb4-47e5-85dd-f073172fc81c",
   "metadata": {},
   "source": [
    "Create `events_clean` by removing any record that \"does not make sense\".\n",
    "Specifically:\n",
    "\n",
    "- Start from the `df_events` DataFrame.\n",
    "- Keep only rows with non-null timestamps, `session_id`, and `product_id`.\n",
    "- Cast price to double; keep `NULL` prices (views/carts can be price-less) and non-negative values only.\n",
    "- Drop dates in the future.\n",
    "- Restrict to valid event types: `view`, `cart`, `purchase`, `remove`.\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "559ae17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes restantes : 42418541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42418541"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Wall time: 18.409 s\n",
      "RSS Δ: -32.27 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 107fd75d0, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 107fd5110, raw_cell=\"# codecell_51ep7v (keep this id for tracking purpo..\" transformed_cell=\"# codecell_51ep7v (keep this id for tracking purpo..\" store_history=False silent=False shell_futures=True cell_id=None> result=42418541>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_51ep7v (keep this id for tracking purposes)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from operator import and_ as AND\n",
    "\n",
    "valid_types = [\"view\", \"cart\", \"purchase\", \"remove\"]\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "# 1. Conversion du prix en double (pour pouvoir comparer les valeurs négatives)\n",
    "# On le fait au début pour simplifier la condition suivante\n",
    "df_typed = df_events.withColumn(\"price\", F.col(\"price\").cast(\"double\"))\n",
    "\n",
    "# 2. Définition des conditions de nettoyage\n",
    "# A. Champs obligatoires non nuls\n",
    "cond_not_null = (\n",
    "    F.col(\"event_time\").isNotNull() & \n",
    "    F.col(\"session_id\").isNotNull() & \n",
    "    F.col(\"product_id\").isNotNull()\n",
    ")\n",
    "\n",
    "# B. Prix valide : Soit NULL (pour les vues/paniers), soit positif ou nul (>= 0)\n",
    "cond_valid_price = (F.col(\"price\").isNull()) | (F.col(\"price\") >= 0)\n",
    "\n",
    "# C. Pas de dates dans le futur (inférieures ou égales à \"maintenant\")\n",
    "cond_not_future = F.col(\"event_time\") <= F.current_timestamp()\n",
    "\n",
    "# D. Types d'événements valides (définis dans la liste valid_types plus haut)\n",
    "cond_valid_type = F.col(\"event_type\").isin(valid_types)\n",
    "\n",
    "# 3. Application de tous les filtres\n",
    "events_clean = df_typed.filter(\n",
    "    cond_not_null & \n",
    "    cond_valid_price & \n",
    "    cond_not_future & \n",
    "    cond_valid_type\n",
    ")\n",
    "\n",
    "# By the time we get to here, \"events_clean\" should conform to the specification above.\n",
    "\n",
    "# Affichage pour vérification\n",
    "print(f\"Nombre de lignes restantes : {events_clean.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94fd335",
   "metadata": {},
   "source": [
    "### 5.2 Cap Silly Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6f0df-6101-4c96-a30a-03b498844a20",
   "metadata": {},
   "source": [
    "Next, let us check some statistics about prices and then decide what we want to do.\n",
    "\n",
    "What is the minimum, maximum, and average price in this database?\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a169cdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 159:====================================================>  (25 + 1) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum: 0.0\n",
      "maximum: 257407.0\n",
      "average: 864.2732006942867\n",
      "======================================\n",
      "Wall time: 9.350 s\n",
      "RSS Δ: +0.02 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 107f4cd50, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 107f4ebd0, raw_cell=\"# codecell_52hg6x (keep this id for tracking purpo..\" transformed_cell=\"# codecell_52hg6x (keep this id for tracking purpo..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_52hg6x (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "# On sélectionne les agrégats en une seule passe pour être efficace\n",
    "stats = events_clean.select(\n",
    "    F.min(\"price\").alias(\"min_p\"),\n",
    "    F.max(\"price\").alias(\"max_p\"),\n",
    "    F.avg(\"price\").alias(\"avg_p\")\n",
    ").collect()[0] # On récupère la première (et unique) ligne de résultat\n",
    "\n",
    "# On assigne les valeurs aux variables demandées\n",
    "minimum = stats[\"min_p\"]\n",
    "maximum = stats[\"max_p\"]\n",
    "average = stats[\"avg_p\"]\n",
    "\n",
    "# By the time we get to here, \"minimum\", \"maximum\", and \"average\" should conform to the specification above.\n",
    "\n",
    "print(f\"minimum: {minimum}\")\n",
    "print(f\"maximum: {maximum}\")\n",
    "print(f\"average: {average}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b48511",
   "metadata": {},
   "source": [
    "Wait, something's not right! \n",
    "The average price is 864.27 but the maximum seems suss...\n",
    "It is possible these high prices are just errors.\n",
    "\n",
    "For simplicity, let us assume a threshold value equal to 100x the average, and remove anything more than that.\n",
    "Filter `events_clean` as described.\n",
    "\n",
    "write some code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9783cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seuil appliqué : 86427.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42351862"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Wall time: 9.156 s\n",
      "RSS Δ: +0.02 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 108034310, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 108035a90, raw_cell=\"# codecell_52bf5d (keep this id for tracking purpo..\" transformed_cell=\"# codecell_52bf5d (keep this id for tracking purpo..\" store_history=False silent=False shell_futures=True cell_id=None> result=42351862>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_52bf5d (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "# 1. Calcul du seuil limite\n",
    "threshold = 100 * average\n",
    "\n",
    "# 2. Application du filtre\n",
    "# On garde : (Prix <= seuil) OU (Prix est NULL)\n",
    "events_clean = events_clean.filter(\n",
    "    (F.col(\"price\") <= threshold) | \n",
    "    (F.col(\"price\").isNull())\n",
    ")\n",
    "\n",
    "print(f\"Seuil appliqué : {threshold:.2f}\")\n",
    "\n",
    "# By the time we get to here, \"events_clean\" should conform to the specification above.\n",
    "\n",
    "events_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33e64b",
   "metadata": {},
   "source": [
    "Good, we still have about 42.4M records, but we've done some basic data cleaning.\n",
    "Let us continue..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3065688-4d49-4567-b7f5-eb866bf52441",
   "metadata": {},
   "source": [
    "### 5.3 Build Tiny Lookup Tables (LKPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b5cb09-baf1-45c4-9b6b-0083c1d86b4b",
   "metadata": {},
   "source": [
    "Create lookup tables that help us connect `events_clean` with the dimension tables we created:\n",
    "\n",
    "- `user_lkp`: (`user_id` → `user_key`) from `dim_user`.\n",
    "- `prod_lkp`: (`product_id` → `product_key`, `brand_key`, `category_key`) from `dim_product`.\n",
    "- `date_lkp`: (`date` → `date_key`) from `dim_date`.\n",
    "- session-to-user bridge: use the raw `df_session` (`session_id`, `user_id`) CSV (not a dimension) to pull `user_id`.\n",
    "\n",
    "**Hint:** These LKPs are just calling `select` from the right sources with the right parameters.\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "056954dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9244421 3022290 166794 32\n",
      "======================================\n",
      "Wall time: 5.284 s\n",
      "RSS Δ: -3.52 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 107fe4c50, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 107fe62d0, raw_cell=\"# codecell_53l2kp (keep this id for tracking purpo..\" transformed_cell=\"# codecell_53l2kp (keep this id for tracking purpo..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_53l2kp (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "# 1. User Lookup : lien entre user_id (naturel) et user_key (surrogate)\n",
    "user_lkp = dim_user.select(\"user_id\", \"user_key\")\n",
    "\n",
    "# 2. Product Lookup : lien entre product_id et toutes les clés dimensionnelles associées\n",
    "prod_lkp = dim_product.select(\"product_id\", \"product_key\", \"brand_key\", \"category_key\")\n",
    "\n",
    "# 3. Date Lookup : lien entre la date calendaire et le date_key (YYYYMMDD)\n",
    "date_lkp = dim_date.select(\"date\", \"date_key\")\n",
    "\n",
    "# 4. Session Bridge : lien entre session_id et user_id\n",
    "# On utilise df_session (la table raw) et on s'assure qu'elle est unique\n",
    "session_bridge = df_session.select(\"session_id\", \"user_id\").distinct()\n",
    "\n",
    "# By the time we get to here, the following variables should conform to the specification above.\n",
    "\n",
    "print(session_bridge.count(), user_lkp.count(), prod_lkp.count(), date_lkp.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b6b4e-0449-4db7-8971-98ceabe29eb9",
   "metadata": {},
   "source": [
    "### 5.4 Join Everything Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad90901",
   "metadata": {},
   "source": [
    "Finally, join everything together to create `fact_events`.\n",
    "Follow the following steps:\n",
    "\n",
    "- Start from `clean events` with these columns: (`event_time`, `event_type`, `session_id`, `product_id`, `price`, `date`).\n",
    "- Join sessions first (to get `user_id`).\n",
    "- Then join product, date, and user.\n",
    "- Join with `dim_user` to find out the birthdate and compute user age at the day of the event in `age_on_event`.\n",
    "- Join with `dim_age` to find the age band based on `age_on_event`.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- You built the LKPs for a reason... use them.\n",
    "- Left, right, or natural joins?\n",
    "\n",
    "The final part above is a bit tricky, so we'll just give you the answer. But you'll need to figure out how it integrates with everything above.\n",
    "\n",
    "```\n",
    "        .withColumn(\"age_on_event\", F.floor(F.months_between(F.col(\"date\"), F.to_date(\"birthdate\"))/12))\n",
    "        .join(\n",
    "           dim_age.select(\"age_key\", \"age_band\", \"min_age\", \"max_age\"),\n",
    "           (\n",
    "               ((F.col(\"age_on_event\") > F.col(\"min_age\"))) &\n",
    "               ((F.col(\"age_on_event\") <= F.col(\"max_age\")))\n",
    "           ),\n",
    "           \"left\"\n",
    "       )\n",
    "```\n",
    "\n",
    "The final result (`fact_events`) should include the following columns:\n",
    "\n",
    "- `date_key`\n",
    "- `user_key`\n",
    "- `age_key`\n",
    "- `product_key`\n",
    "- `brand_key`\n",
    "- `category_key`\n",
    "- `session_id`\n",
    "- `event_time`\n",
    "- `event_type`\n",
    "- `price`\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4660de20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 16:01:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lignes finales dans la table de faits : 42351862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 16:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:01:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+-----------+---------+------------+--------------------+-------------------+----------+------+\n",
      "|date_key|   user_key|age_key|product_key|brand_key|category_key|          session_id|         event_time|event_type| price|\n",
      "+--------+-----------+-------+-----------+---------+------------+--------------------+-------------------+----------+------+\n",
      "|20191031|42949886615|      4|      14079|     2555|           3|0b63f487-9146-4ec...|2019-10-31 14:05:12|      view| 38.35|\n",
      "|20191018|17180086555|      1|      22906|     3244|           3|40d01ba8-f8cc-4a9...|2019-10-18 05:30:27|      view|  51.4|\n",
      "|20191008|42949945300|      2|       1197|     NULL|           8|38609af5-1ece-422...|2019-10-08 20:03:41|      view|771.94|\n",
      "|20191018|17180053625|   NULL|        223|     2702|           8|22f1098e-f80f-40f...|2019-10-18 05:30:28|      view| 344.9|\n",
      "|20191002|     324788|      4|     149294|     NULL|        NULL|1a3583db-bbe0-40e...|2019-10-02 11:07:43|      view| 72.64|\n",
      "+--------+-----------+-------+-----------+---------+------------+--------------------+-------------------+----------+------+\n",
      "only showing top 5 rows\n",
      "======================================\n",
      "Wall time: 52.668 s\n",
      "RSS Δ: -20.86 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 10801ab90, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 10801ad10, raw_cell=\"# codecell_54aaaa (keep this id for tracking purpo..\" transformed_cell=\"# codecell_54aaaa (keep this id for tracking purpo..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_54aaaa (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "# 1. Préparation de la base\n",
    "# On ajoute une colonne 'date' (type Date) à events_clean pour pouvoir joindre avec dim_date\n",
    "base_events = events_clean.withColumn(\"date\", F.to_date(\"event_time\"))\n",
    "\n",
    "# 2. Chaînage des jointures\n",
    "fact_events_temp = base_events \\\n",
    "    .join(session_bridge, on=\"session_id\", how=\"left\") \\\n",
    "    .join(prod_lkp, on=\"product_id\", how=\"left\") \\\n",
    "    .join(date_lkp, on=\"date\", how=\"left\") \\\n",
    "    .join(dim_user, on=\"user_id\", how=\"left\") # On joint dim_user (pas user_lkp) pour récupérer 'birthdate'\n",
    "\n",
    "# 3. Calcul de l'âge et jointure avec dim_age\n",
    "# (On utilise le snippet fourni dans l'énoncé)\n",
    "fact_events_w_age = fact_events_temp \\\n",
    "    .withColumn(\"age_on_event\", F.floor(F.months_between(F.col(\"date\"), F.to_date(\"birthdate\"))/12)) \\\n",
    "    .join(\n",
    "        dim_age.select(\"age_key\", \"age_band\", \"min_age\", \"max_age\"),\n",
    "        (\n",
    "            ((F.col(\"age_on_event\") > F.col(\"min_age\"))) &\n",
    "            ((F.col(\"age_on_event\") <= F.col(\"max_age\")))\n",
    "        ),\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "# 4. Sélection finale des colonnes (mise en forme)\n",
    "fact_events = fact_events_w_age.select(\n",
    "    F.col(\"date_key\"),\n",
    "    F.col(\"user_key\"),\n",
    "    F.col(\"age_key\"),\n",
    "    F.col(\"product_key\"),\n",
    "    F.col(\"brand_key\"),\n",
    "    F.col(\"category_key\"),\n",
    "    F.col(\"session_id\"),\n",
    "    F.col(\"event_time\"),\n",
    "    F.col(\"event_type\"),\n",
    "    F.col(\"price\")\n",
    ")\n",
    "\n",
    "# By the time we get to here, \"fact_events\" should conform to the specification above.\n",
    "\n",
    "# Affichage pour contrôle\n",
    "print(f\"Lignes finales dans la table de faits : {fact_events.count()}\")\n",
    "fact_events.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abdfc6c",
   "metadata": {},
   "source": [
    "Congrats, you've done it!\n",
    "You've created the fact table successfuly! 🚀\n",
    "\n",
    "Here is the summary of the schema:\n",
    "\n",
    "- `date_key` (FK → `dim_date`)\n",
    "- `user_key` (FK → `dim_user`)\n",
    "- `age_key`  (FK → `dim_age`)\n",
    "- `product_key` (FK → `dim_product`)\n",
    "- `brand_key` (FK → `dim_brand`)\n",
    "- `category_key` (FK → `dim_category`)\n",
    "- `session_id` (STRING, business key, kept directly in this table)\n",
    "- `event_time` (TIMESTAMP)\n",
    "- `event_tpe` (STRING)\n",
    "- `price` (DOUBLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa5c0a-b7ab-4340-9b05-1dc615444254",
   "metadata": {},
   "source": [
    "## 6. Export the Fact Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ebb4ea",
   "metadata": {},
   "source": [
    "You now have a shiny `fact_events` table!\n",
    "But how should you store it?\n",
    "(Remember our discussion in class about row vs. column representations?)\n",
    "\n",
    "Let's store `fact_events` in a few different ways and compare data sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb22a99-f504-4487-97c3-324554333091",
   "metadata": {},
   "source": [
    "First, let's try writing out as CSV files, both compressed and uncompressed, per below.\n",
    "\n",
    "Note that in Spark, we specify the output _directory_, which is then populated with many \"part\" files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb42721a-f950-411d-93ce-2069629f019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:04:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:05:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:05:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:05:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:05:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:05:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:05:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:05:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:05:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:05:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:05:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:05:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:05:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:06:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fact_events.write.mode(\"overwrite\").option(\"header\", True).csv(BASE_DIR + \"/fact_events.csv\")\n",
    "fact_events.write.mode(\"overwrite\").option(\"header\", True).option(\"compression\", \"snappy\").csv(BASE_DIR + \"/fact_events.csv.snappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb2541-f412-437f-8b1f-d973ab722ab5",
   "metadata": {},
   "source": [
    "Let's then try Parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e9721b6-d435-4438-bf06-131a8d0a4384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/29 16:08:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fact_events.write.mode(\"overwrite\").parquet(BASE_DIR + \"/fact_events.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d8161-114a-4896-b5b9-aa7a27c02e76",
   "metadata": {},
   "source": [
    "Let's compare the output sizes using the following bit of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "453bd520-4a21-4161-b6c2-6758bb6546e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lorenzo/Documents/Cours/E4FD/Data-Engineering/Lab2/lab2Assignment/fact_events.csv: 4.4 GB\n",
      "/Users/lorenzo/Documents/Cours/E4FD/Data-Engineering/Lab2/lab2Assignment/fact_events.csv.snappy: 1.2 GB\n",
      "/Users/lorenzo/Documents/Cours/E4FD/Data-Engineering/Lab2/lab2Assignment/fact_events.parquet: 1.0 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for f in [BASE_DIR + \"/fact_events.csv\", BASE_DIR + \"/fact_events.csv.snappy\", BASE_DIR + \"/fact_events.parquet\"]:\n",
    "    try:\n",
    "        size = sum(os.path.getsize(os.path.join(dp, fn))\n",
    "                   for dp, dn, filenames in os.walk(f)\n",
    "                   for fn in filenames)\n",
    "        print(f\"{f}: {size/(1024*1024*1024):.1f} GB\")\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d796fa9-cad5-4ca6-b6cd-cbae63cdc96e",
   "metadata": {},
   "source": [
    "**your answers below!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56df9e6-a6bd-4ebd-a692-63c4bd590475",
   "metadata": {},
   "source": [
    "// qcell_6a9876 (keep this id for tracking purposes)\n",
    "\n",
    "- **Size of CSV output, no compression:** 4.4  GB\n",
    "- **Size of CSV output, Snappy compression:** 1.2 GB\n",
    "- **Size of Parquet output:** 1.0 GB "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b7369-6ce3-455a-917b-bc002e112da4",
   "metadata": {},
   "source": [
    "**Answer the following question:**\n",
    "\n",
    "Q6.1 Why is columnar storage (Parquet) usually much smaller?\n",
    "\n",
    "Q6.2 Which format is better for analytical queries and why?\n",
    "\n",
    "**your answers below!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad1b29b-375e-4143-b844-cd40c3183e0f",
   "metadata": {},
   "source": [
    "// qcell_6b1234 (keep this id for tracking purposes)\n",
    "\n",
    "**Q6.1 Answer:**\n",
    "\n",
    "Le stockage colonnaire est beaucoup plus compact car il regroupe les données par colonne, créant des blocs de valeurs homogènes. Contrairement au stockage par ligne où les types varient constamment, cette avantage permet aux algorithmes de compression (comme Snappy ou RLE) de réduire l'espace disque, par exemple en ne stockant qu'une seule fois une valeur qui se répète 1000 fois de suite.\n",
    "\n",
    "**Q6.2 Answer:**\n",
    "\n",
    "Le format Parquet est le meilleur pour l'analytique car il minimise les lectures disques (I/O) grâce à la \"projection de colonnes\", qui permet de ne charger en mémoire que les colonnes utiles à la requête. Cela permet aux métadonnées d'ignorer des blocs entiers de fichiers ne correspondant pas aux filtres. Cela permet à Spark de traiter des agrégations sur des milliards de lignes très rapidement en évitant de lire des téraoctets de données inutiles, ce qui est impossible avec un format ligne comme le CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5ab3d-73dd-4bd9-9722-4f07e6b913c0",
   "metadata": {},
   "source": [
    "## 7. Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568bb473",
   "metadata": {},
   "source": [
    "Details about the Submission of this assignment are outlined in the helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89841125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Wall time: 4.429 s\n",
      "RSS Δ: +1.92 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 107fe4550, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 107fe4350, raw_cell=\"spark.stop()\n",
       "\" transformed_cell=\"spark.stop()\n",
       "\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c95cb45",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "- This notebook with all code cells executed.\n",
    "- A brief `REPORT.md` with: inputs, assumptions, plan screenshots, quality results, and performance choices.\n",
    "- Output folder with Parquet sample (≤20 MB).\n",
    "\n",
    "## Evaluation\n",
    "- Correctness and clarity of pipeline (40%).\n",
    "- Data‑quality gates and rationale (20%).\n",
    "- Performance reasoning and plan analysis (20%).\n",
    "- Reproducibility and organization (20%).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46acfb1",
   "metadata": {},
   "source": [
    "## Performance notes\n",
    "- Record `spark.sql.shuffle.partitions` and justify your value.\n",
    "- Show one example of avoiding UDFs by using built‑ins.\n",
    "- If you use broadcast join, explain why it is safe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5583ac",
   "metadata": {},
   "source": [
    "## Reproducibility checklist\n",
    "- List Spark version and key configs.\n",
    "- Fix time zone to UTC.\n",
    "- Control randomness if used.\n",
    "- Provide exact commands to run the notebook end‑to‑end.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de1-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
